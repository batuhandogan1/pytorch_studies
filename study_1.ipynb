{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Create"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set weight and bias\n",
    "weight = 0.9\n",
    "bias = 0.2\n",
    "\n",
    "X = torch.arange(start=0.0, end=1, step=0.001).unsqueeze(dim=1)\n",
    "\n",
    "# Get labels for y = weight * X + bias\n",
    "y = weight * X + bias\n",
    "\n",
    "train_test_split = int(0.8 * len(X))\n",
    "\n",
    "X_train, y_train = X[:train_test_split], y[:train_test_split]\n",
    "X_test, y_test = X[train_test_split:], y[train_test_split:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_predictions(train_data=X_train, \n",
    "                     train_labels=y_train, \n",
    "                     test_data=X_test, \n",
    "                     test_labels=y_test, \n",
    "                     predictions=None):\n",
    "  \"\"\"\n",
    "  Plots training data, test data and compares predictions.\n",
    "  \"\"\"\n",
    "  plt.figure(figsize=(10, 7))\n",
    "\n",
    "  # Plot training data in blue\n",
    "  plt.scatter(train_data, train_labels, c=\"b\", s=4, label=\"Training data\")\n",
    "  \n",
    "  # Plot test data in green\n",
    "  plt.scatter(test_data, test_labels, c=\"g\", s=4, label=\"Testing data\")\n",
    "\n",
    "  if predictions is not None:\n",
    "    # Plot the predictions in red (predictions were made on the test data)\n",
    "    plt.scatter(test_data, predictions, c=\"r\", s=4, label=\"Predictions\")\n",
    "\n",
    "  # Show the legend\n",
    "  plt.legend(prop={\"size\": 14})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x700 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_predictions(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build a Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "else:\n",
    "    device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegressionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear_layer = nn.Linear(in_features=1, out_features=1)\n",
    "\n",
    "    def forward(self, X: torch.Tensor) -> torch.Tensor:\n",
    "        return self.linear_layer(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('linear_layer.weight', tensor([[0.7348]])),\n",
       "             ('linear_layer.bias', tensor([-0.5476]))])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(86)\n",
    "model = LinearRegressionModel()\n",
    "\n",
    "model.state_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Optimization and Loss Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegressionModel(\n",
       "  (linear_layer): Linear(in_features=1, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.00001, momentum=0.9)\n",
    "loss_fn = nn.L1Loss()\n",
    "\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x700 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with torch.inference_mode():\n",
    "    \n",
    "    test_preds = model(X_test)\n",
    "\n",
    "    plot_predictions(predictions=test_preds.cpu())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 1 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 2 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 3 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 4 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 5 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 6 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 7 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 8 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 9 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 10 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 11 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 12 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 13 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 14 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 15 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 16 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 17 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 18 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 19 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 20 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 21 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 22 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 23 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 24 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 25 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 26 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 27 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 28 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 29 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 30 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 31 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 32 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 33 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 34 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 35 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 36 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 37 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 38 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 39 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 40 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 41 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 42 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 43 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 44 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 45 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 46 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 47 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 48 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 49 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 50 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 51 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 52 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 53 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 54 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 55 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 56 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 57 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 58 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 59 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 60 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 61 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 62 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 63 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 64 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 65 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 66 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 67 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 68 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 69 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 70 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 71 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 72 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 73 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 74 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 75 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 76 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 77 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 78 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 79 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 80 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 81 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 82 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 83 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 84 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 85 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 86 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 87 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 88 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 89 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 90 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 91 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 92 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 93 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 94 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 95 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 96 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 97 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 98 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 99 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 100 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 101 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 102 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 103 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 104 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 105 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 106 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 107 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 108 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 109 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 110 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 111 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 112 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 113 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 114 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 115 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 116 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 117 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 118 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 119 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 120 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 121 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 122 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 123 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 124 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 125 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 126 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 127 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 128 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 129 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 130 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 131 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 132 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 133 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 134 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 135 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 136 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 137 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 138 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 139 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 140 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 141 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 142 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 143 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 144 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 145 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 146 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 147 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 148 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 149 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 150 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 151 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 152 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 153 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 154 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 155 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 156 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 157 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 158 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 159 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 160 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 161 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 162 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 163 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 164 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 165 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 166 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 167 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 168 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 169 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 170 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 171 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 172 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 173 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 174 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 175 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 176 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 177 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 178 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 179 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 180 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 181 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 182 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 183 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 184 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 185 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 186 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 187 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 188 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 189 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 190 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 191 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 192 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 193 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 194 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 195 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 196 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 197 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 198 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 199 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 200 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 201 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 202 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 203 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 204 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 205 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 206 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 207 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 208 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 209 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 210 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 211 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 212 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 213 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 214 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 215 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 216 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 217 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 218 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 219 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 220 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 221 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 222 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 223 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 224 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 225 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 226 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 227 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 228 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 229 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 230 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 231 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 232 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 233 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 234 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 235 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 236 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 237 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 238 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 239 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 240 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 241 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 242 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 243 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 244 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 245 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 246 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 247 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 248 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 249 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 250 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 251 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 252 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 253 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 254 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 255 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 256 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 257 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 258 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 259 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 260 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 261 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 262 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 263 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 264 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 265 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 266 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 267 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 268 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 269 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 270 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 271 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 272 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 273 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 274 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 275 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 276 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 277 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 278 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 279 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 280 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 281 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 282 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 283 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 284 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 285 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 286 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 287 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 288 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 289 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 290 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 291 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 292 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 293 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 294 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 295 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 296 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 297 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 298 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 299 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 300 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 301 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 302 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 303 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 304 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 305 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 306 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 307 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 308 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 309 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 310 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 311 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 312 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 313 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 314 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 315 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 316 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 317 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 318 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 319 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 320 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 321 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 322 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 323 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 324 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 325 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 326 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 327 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 328 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 329 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 330 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 331 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 332 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 333 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 334 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 335 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 336 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 337 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 338 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 339 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 340 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 341 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 342 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 343 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 344 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 345 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 346 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 347 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 348 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 349 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 350 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 351 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 352 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 353 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 354 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 355 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 356 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 357 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 358 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 359 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 360 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 361 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 362 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 363 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 364 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 365 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 366 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 367 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 368 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 369 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 370 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 371 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 372 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 373 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 374 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 375 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 376 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 377 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 378 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 379 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 380 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 381 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 382 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 383 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 384 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 385 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 386 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 387 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 388 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 389 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 390 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 391 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 392 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 393 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 394 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 395 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 396 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 397 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 398 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 399 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 400 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 401 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 402 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 403 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 404 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 405 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 406 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 407 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 408 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 409 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 410 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 411 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 412 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 413 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 414 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 415 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 416 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 417 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 418 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 419 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 420 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 421 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 422 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 423 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 424 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 425 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 426 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 427 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 428 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 429 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 430 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 431 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 432 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 433 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 434 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 435 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 436 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 437 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 438 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 439 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 440 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 441 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 442 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 443 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 444 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 445 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 446 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 447 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 448 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 449 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 450 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 451 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 452 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 453 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 454 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 455 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 456 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 457 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 458 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 459 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 460 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 461 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 462 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 463 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 464 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 465 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 466 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 467 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 468 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 469 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 470 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 471 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 472 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 473 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 474 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 475 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 476 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 477 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 478 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 479 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 480 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 481 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 482 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 483 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 484 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 485 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 486 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 487 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 488 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 489 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 490 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 491 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 492 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 493 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 494 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 495 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 496 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 497 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 498 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 499 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 500 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 501 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 502 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 503 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 504 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 505 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 506 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 507 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 508 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 509 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 510 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 511 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 512 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 513 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 514 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 515 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 516 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 517 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 518 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 519 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 520 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 521 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 522 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 523 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 524 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 525 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 526 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 527 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 528 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 529 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 530 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 531 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 532 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 533 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 534 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 535 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 536 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 537 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 538 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 539 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 540 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 541 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 542 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 543 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 544 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 545 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 546 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 547 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 548 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 549 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 550 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 551 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 552 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 553 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 554 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 555 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 556 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 557 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 558 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 559 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 560 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 561 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 562 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 563 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 564 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 565 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 566 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 567 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 568 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 569 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 570 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 571 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 572 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 573 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 574 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 575 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 576 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 577 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 578 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 579 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 580 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 581 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 582 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 583 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 584 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 585 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 586 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 587 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 588 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 589 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 590 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 591 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 592 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 593 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 594 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 595 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 596 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 597 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 598 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 599 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 600 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 601 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 602 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 603 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 604 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 605 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 606 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 607 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 608 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 609 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 610 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 611 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 612 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 613 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 614 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 615 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 616 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 617 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 618 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 619 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 620 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 621 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 622 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 623 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 624 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 625 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 626 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 627 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 628 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 629 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 630 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 631 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 632 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 633 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 634 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 635 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 636 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 637 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 638 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 639 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 640 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 641 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 642 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 643 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 644 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 645 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 646 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 647 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 648 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 649 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 650 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 651 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 652 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 653 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 654 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 655 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 656 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 657 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 658 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 659 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 660 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 661 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 662 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 663 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 664 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 665 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 666 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 667 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 668 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 669 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 670 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 671 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 672 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 673 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 674 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 675 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 676 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 677 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 678 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 679 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 680 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 681 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 682 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 683 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 684 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 685 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 686 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 687 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 688 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 689 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 690 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 691 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 692 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 693 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 694 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 695 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 696 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 697 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 698 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 699 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 700 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 701 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 702 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 703 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 704 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 705 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 706 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 707 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 708 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 709 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 710 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 711 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 712 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 713 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 714 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 715 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 716 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 717 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 718 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 719 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 720 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 721 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 722 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 723 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 724 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 725 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 726 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 727 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 728 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 729 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 730 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 731 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 732 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 733 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 734 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 735 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 736 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 737 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 738 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 739 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 740 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 741 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 742 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 743 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 744 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 745 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 746 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 747 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 748 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 749 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 750 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 751 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 752 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 753 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 754 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 755 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 756 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 757 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 758 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 759 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 760 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 761 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 762 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 763 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 764 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 765 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 766 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 767 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 768 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 769 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 770 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 771 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 772 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 773 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 774 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 775 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 776 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 777 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 778 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 779 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 780 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 781 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 782 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 783 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 784 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 785 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 786 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 787 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 788 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 789 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 790 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 791 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 792 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 793 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 794 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 795 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 796 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 797 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 798 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 799 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 800 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 801 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 802 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 803 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 804 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 805 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 806 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 807 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 808 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 809 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 810 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 811 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 812 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 813 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 814 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 815 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 816 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 817 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 818 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 819 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 820 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 821 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 822 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 823 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 824 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 825 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 826 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 827 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 828 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 829 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 830 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 831 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 832 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 833 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 834 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 835 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 836 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 837 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 838 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 839 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 840 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 841 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 842 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 843 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 844 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 845 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 846 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 847 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 848 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 849 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 850 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 851 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 852 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 853 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 854 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 855 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 856 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 857 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 858 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 859 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 860 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 861 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 862 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 863 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 864 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 865 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 866 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 867 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 868 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 869 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 870 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 871 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 872 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 873 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 874 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 875 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 876 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 877 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 878 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 879 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 880 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 881 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 882 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 883 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 884 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 885 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 886 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 887 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 888 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 889 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 890 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 891 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 892 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 893 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 894 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 895 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 896 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 897 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 898 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 899 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 900 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 901 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 902 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 903 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 904 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 905 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 906 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 907 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 908 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 909 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 910 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 911 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 912 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 913 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 914 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 915 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 916 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 917 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 918 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 919 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 920 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 921 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 922 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 923 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 924 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 925 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 926 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 927 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 928 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 929 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 930 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 931 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 932 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 933 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 934 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 935 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 936 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 937 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 938 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 939 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 940 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 941 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 942 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 943 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 944 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 945 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 946 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 947 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 948 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 949 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 950 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 951 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 952 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 953 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 954 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 955 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 956 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 957 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 958 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 959 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 960 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 961 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 962 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 963 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 964 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 965 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 966 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 967 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 968 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 969 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 970 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 971 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 972 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 973 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 974 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 975 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 976 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 977 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 978 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 979 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 980 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 981 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 982 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 983 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 984 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 985 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 986 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 987 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 988 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 989 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 990 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 991 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 992 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 993 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 994 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 995 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 996 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 997 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 998 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 999 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 1000 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 1001 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 1002 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 1003 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 1004 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 1005 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 1006 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 1007 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 1008 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 1009 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 1010 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 1011 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 1012 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 1013 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 1014 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 1015 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 1016 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 1017 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 1018 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 1019 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 1020 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 1021 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 1022 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 1023 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 1024 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 1025 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 1026 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 1027 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 1028 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 1029 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 1030 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 1031 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 1032 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 1033 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 1034 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 1035 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 1036 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 1037 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 1038 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 1039 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 1040 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 1041 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 1042 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 1043 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 1044 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 1045 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 1046 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 1047 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 1048 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 1049 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 1050 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 1051 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 1052 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 1053 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 1054 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 1055 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 1056 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 1057 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 1058 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 1059 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 1060 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 1061 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 1062 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 1063 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 1064 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 1065 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 1066 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 1067 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 1068 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 1069 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 1070 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 1071 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 1072 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 1073 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 1074 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 1075 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 1076 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 1077 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 1078 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 1079 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 1080 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 1081 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 1082 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 1083 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 1084 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 1085 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 1086 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 1087 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 1088 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 1089 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 1090 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 1091 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 1092 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 1093 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 1094 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 1095 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 1096 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 1097 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 1098 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 1099 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 1100 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 1101 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 1102 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 1103 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 1104 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 1105 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 1106 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 1107 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 1108 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 1109 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 1110 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 1111 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 1112 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 1113 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 1114 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 1115 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 1116 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 1117 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 1118 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 1119 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 1120 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 1121 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 1122 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 1123 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 1124 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 1125 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 1126 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 1127 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 1128 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 1129 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 1130 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 1131 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 1132 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 1133 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 1134 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 1135 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 1136 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 1137 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 1138 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 1139 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 1140 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 1141 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 1142 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 1143 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 1144 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 1145 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 1146 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 1147 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 1148 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 1149 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 1150 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 1151 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 1152 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 1153 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 1154 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 1155 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 1156 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 1157 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 1158 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 1159 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 1160 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 1161 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 1162 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 1163 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 1164 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 1165 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 1166 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 1167 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 1168 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 1169 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 1170 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 1171 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 1172 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 1173 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 1174 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 1175 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 1176 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 1177 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 1178 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 1179 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 1180 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 1181 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 1182 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 1183 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 1184 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 1185 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 1186 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 1187 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 1188 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 1189 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 1190 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 1191 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 1192 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 1193 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 1194 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 1195 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 1196 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 1197 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 1198 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 1199 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 1200 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 1201 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 1202 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 1203 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 1204 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 1205 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 1206 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 1207 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 1208 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 1209 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 1210 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 1211 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 1212 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 1213 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 1214 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 1215 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 1216 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 1217 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 1218 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 1219 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 1220 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 1221 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 1222 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 1223 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 1224 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 1225 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 1226 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 1227 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 1228 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 1229 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 1230 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 1231 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 1232 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 1233 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 1234 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 1235 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 1236 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 1237 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 1238 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 1239 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 1240 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 1241 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 1242 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 1243 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 1244 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 1245 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 1246 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 1247 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 1248 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 1249 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 1250 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 1251 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 1252 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 1253 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 1254 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 1255 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 1256 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 1257 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 1258 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 1259 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 1260 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 1261 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 1262 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 1263 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 1264 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 1265 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 1266 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 1267 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 1268 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 1269 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 1270 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 1271 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 1272 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 1273 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 1274 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 1275 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 1276 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 1277 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 1278 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 1279 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 1280 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 1281 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 1282 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 1283 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 1284 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 1285 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 1286 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 1287 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 1288 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 1289 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 1290 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 1291 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 1292 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 1293 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 1294 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 1295 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 1296 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 1297 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 1298 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 1299 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 1300 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 1301 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 1302 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 1303 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 1304 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 1305 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 1306 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 1307 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 1308 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 1309 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 1310 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 1311 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 1312 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 1313 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 1314 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 1315 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 1316 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 1317 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 1318 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 1319 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 1320 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 1321 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 1322 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 1323 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 1324 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 1325 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 1326 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 1327 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 1328 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 1329 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 1330 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 1331 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 1332 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 1333 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 1334 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 1335 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 1336 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 1337 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 1338 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 1339 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 1340 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 1341 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 1342 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 1343 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 1344 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 1345 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 1346 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 1347 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 1348 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 1349 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 1350 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 1351 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 1352 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 1353 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 1354 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 1355 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 1356 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 1357 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 1358 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 1359 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 1360 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 1361 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 1362 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 1363 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 1364 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 1365 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 1366 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 1367 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 1368 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 1369 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 1370 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 1371 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 1372 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 1373 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 1374 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 1375 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 1376 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 1377 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 1378 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 1379 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 1380 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 1381 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 1382 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 1383 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 1384 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 1385 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 1386 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 1387 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 1388 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 1389 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 1390 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 1391 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 1392 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 1393 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 1394 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 1395 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 1396 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 1397 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 1398 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 1399 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 1400 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 1401 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 1402 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 1403 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 1404 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 1405 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 1406 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 1407 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 1408 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 1409 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 1410 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 1411 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 1412 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 1413 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 1414 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 1415 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 1416 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 1417 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 1418 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 1419 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 1420 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 1421 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 1422 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 1423 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 1424 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 1425 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 1426 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 1427 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 1428 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 1429 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 1430 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 1431 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 1432 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 1433 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 1434 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 1435 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 1436 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 1437 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 1438 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 1439 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 1440 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 1441 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 1442 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 1443 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 1444 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 1445 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 1446 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 1447 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 1448 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 1449 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 1450 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 1451 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 1452 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 1453 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 1454 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 1455 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 1456 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 1457 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 1458 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 1459 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 1460 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 1461 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 1462 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 1463 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 1464 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 1465 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 1466 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 1467 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 1468 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 1469 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 1470 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 1471 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 1472 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 1473 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 1474 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 1475 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 1476 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 1477 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 1478 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 1479 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 1480 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 1481 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 1482 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 1483 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 1484 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 1485 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 1486 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 1487 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 1488 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 1489 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 1490 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 1491 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 1492 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 1493 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 1494 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 1495 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 1496 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 1497 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 1498 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 1499 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 1500 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 1501 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 1502 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 1503 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 1504 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 1505 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 1506 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 1507 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 1508 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 1509 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 1510 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 1511 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 1512 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 1513 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 1514 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 1515 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 1516 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 1517 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 1518 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 1519 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 1520 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 1521 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 1522 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 1523 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 1524 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 1525 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 1526 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 1527 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 1528 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 1529 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 1530 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 1531 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 1532 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 1533 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 1534 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 1535 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 1536 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 1537 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 1538 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 1539 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 1540 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 1541 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 1542 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 1543 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 1544 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 1545 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 1546 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 1547 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 1548 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 1549 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 1550 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 1551 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 1552 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 1553 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 1554 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 1555 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 1556 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 1557 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 1558 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 1559 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 1560 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 1561 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 1562 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 1563 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 1564 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 1565 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 1566 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 1567 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 1568 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 1569 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 1570 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 1571 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 1572 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 1573 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 1574 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 1575 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 1576 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 1577 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 1578 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 1579 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 1580 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 1581 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 1582 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 1583 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 1584 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 1585 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 1586 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 1587 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 1588 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 1589 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 1590 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 1591 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 1592 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 1593 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 1594 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 1595 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 1596 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 1597 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 1598 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 1599 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 1600 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 1601 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 1602 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 1603 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 1604 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 1605 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 1606 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 1607 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 1608 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 1609 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 1610 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 1611 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 1612 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 1613 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 1614 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 1615 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 1616 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 1617 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 1618 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 1619 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 1620 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 1621 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 1622 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 1623 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 1624 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 1625 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 1626 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 1627 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 1628 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 1629 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 1630 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 1631 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 1632 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 1633 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 1634 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 1635 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 1636 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 1637 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 1638 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 1639 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 1640 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 1641 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 1642 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 1643 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 1644 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 1645 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 1646 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 1647 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 1648 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 1649 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 1650 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 1651 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 1652 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 1653 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 1654 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 1655 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 1656 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 1657 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 1658 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 1659 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 1660 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 1661 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 1662 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 1663 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 1664 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 1665 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 1666 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 1667 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 1668 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 1669 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 1670 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 1671 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 1672 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 1673 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 1674 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 1675 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 1676 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 1677 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 1678 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 1679 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 1680 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 1681 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 1682 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 1683 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 1684 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 1685 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 1686 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 1687 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 1688 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 1689 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 1690 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 1691 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 1692 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 1693 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 1694 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 1695 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 1696 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 1697 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 1698 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 1699 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 1700 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 1701 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 1702 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 1703 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 1704 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 1705 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 1706 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 1707 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 1708 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 1709 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 1710 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 1711 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 1712 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 1713 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 1714 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 1715 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 1716 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 1717 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 1718 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 1719 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 1720 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 1721 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 1722 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 1723 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 1724 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 1725 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 1726 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 1727 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 1728 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 1729 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 1730 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 1731 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 1732 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 1733 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 1734 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 1735 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 1736 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 1737 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 1738 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 1739 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 1740 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 1741 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 1742 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 1743 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 1744 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 1745 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 1746 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 1747 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 1748 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 1749 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 1750 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 1751 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 1752 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 1753 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 1754 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 1755 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 1756 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 1757 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 1758 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 1759 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 1760 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 1761 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 1762 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 1763 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 1764 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 1765 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 1766 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 1767 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 1768 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 1769 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 1770 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 1771 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 1772 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 1773 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 1774 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 1775 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 1776 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 1777 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 1778 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 1779 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 1780 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 1781 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 1782 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 1783 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 1784 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 1785 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 1786 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 1787 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 1788 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 1789 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 1790 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 1791 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 1792 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 1793 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 1794 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 1795 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 1796 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 1797 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 1798 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 1799 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 1800 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 1801 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 1802 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 1803 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 1804 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 1805 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 1806 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 1807 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 1808 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 1809 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 1810 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 1811 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 1812 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 1813 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 1814 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 1815 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 1816 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 1817 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 1818 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 1819 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 1820 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 1821 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 1822 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 1823 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 1824 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 1825 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 1826 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 1827 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 1828 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 1829 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 1830 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 1831 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 1832 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 1833 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 1834 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 1835 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 1836 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 1837 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 1838 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 1839 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 1840 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 1841 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 1842 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 1843 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 1844 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 1845 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 1846 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 1847 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 1848 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 1849 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 1850 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 1851 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 1852 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 1853 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 1854 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 1855 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 1856 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 1857 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 1858 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 1859 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 1860 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 1861 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 1862 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 1863 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 1864 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 1865 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 1866 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 1867 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 1868 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 1869 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 1870 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 1871 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 1872 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 1873 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 1874 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 1875 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 1876 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 1877 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 1878 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 1879 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 1880 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 1881 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 1882 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 1883 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 1884 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 1885 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 1886 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 1887 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 1888 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 1889 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 1890 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 1891 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 1892 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 1893 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 1894 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 1895 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 1896 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 1897 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 1898 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 1899 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 1900 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 1901 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 1902 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 1903 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 1904 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 1905 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 1906 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 1907 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 1908 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 1909 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 1910 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 1911 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 1912 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 1913 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 1914 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 1915 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 1916 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 1917 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 1918 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 1919 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 1920 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 1921 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 1922 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 1923 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 1924 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 1925 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 1926 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 1927 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 1928 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 1929 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 1930 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 1931 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 1932 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 1933 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 1934 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 1935 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 1936 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 1937 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 1938 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 1939 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 1940 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 1941 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 1942 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 1943 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 1944 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 1945 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 1946 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 1947 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 1948 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 1949 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 1950 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 1951 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 1952 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 1953 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 1954 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 1955 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 1956 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 1957 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 1958 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 1959 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 1960 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 1961 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 1962 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 1963 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 1964 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 1965 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 1966 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 1967 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 1968 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 1969 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 1970 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 1971 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 1972 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 1973 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 1974 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 1975 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 1976 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 1977 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 1978 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 1979 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 1980 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 1981 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 1982 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 1983 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 1984 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 1985 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 1986 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 1987 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 1988 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 1989 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 1990 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 1991 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 1992 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 1993 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 1994 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 1995 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 1996 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 1997 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 1998 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 1999 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 2000 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 2001 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 2002 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 2003 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 2004 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 2005 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 2006 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 2007 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 2008 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 2009 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 2010 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 2011 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 2012 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 2013 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 2014 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 2015 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 2016 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 2017 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 2018 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 2019 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 2020 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 2021 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 2022 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 2023 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 2024 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 2025 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 2026 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 2027 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 2028 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 2029 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 2030 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 2031 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 2032 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 2033 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 2034 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 2035 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 2036 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 2037 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 2038 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 2039 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 2040 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 2041 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 2042 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 2043 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 2044 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 2045 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 2046 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 2047 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 2048 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 2049 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 2050 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 2051 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 2052 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 2053 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 2054 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 2055 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 2056 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 2057 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 2058 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 2059 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 2060 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 2061 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 2062 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 2063 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 2064 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 2065 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 2066 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 2067 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 2068 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 2069 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 2070 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 2071 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 2072 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 2073 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 2074 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 2075 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 2076 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 2077 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 2078 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 2079 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 2080 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 2081 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 2082 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 2083 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 2084 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 2085 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 2086 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 2087 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 2088 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 2089 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 2090 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 2091 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 2092 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 2093 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 2094 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 2095 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 2096 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 2097 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 2098 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 2099 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 2100 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 2101 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 2102 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 2103 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 2104 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 2105 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 2106 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 2107 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 2108 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 2109 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 2110 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 2111 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 2112 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 2113 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 2114 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 2115 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 2116 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 2117 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 2118 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 2119 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 2120 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 2121 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 2122 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 2123 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 2124 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 2125 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 2126 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 2127 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 2128 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 2129 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 2130 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 2131 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 2132 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 2133 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 2134 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 2135 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 2136 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 2137 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 2138 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 2139 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 2140 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 2141 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 2142 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 2143 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 2144 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 2145 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 2146 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 2147 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 2148 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 2149 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 2150 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 2151 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 2152 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 2153 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 2154 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 2155 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 2156 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 2157 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 2158 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 2159 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 2160 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 2161 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 2162 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 2163 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 2164 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 2165 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 2166 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 2167 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 2168 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 2169 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 2170 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 2171 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 2172 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 2173 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 2174 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 2175 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 2176 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 2177 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 2178 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 2179 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 2180 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 2181 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 2182 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 2183 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 2184 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 2185 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 2186 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 2187 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 2188 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 2189 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 2190 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 2191 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 2192 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 2193 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 2194 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 2195 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 2196 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 2197 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 2198 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 2199 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 2200 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 2201 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 2202 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 2203 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 2204 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 2205 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 2206 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 2207 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 2208 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 2209 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 2210 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 2211 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 2212 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 2213 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 2214 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 2215 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 2216 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 2217 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 2218 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 2219 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 2220 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 2221 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 2222 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 2223 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 2224 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 2225 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 2226 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 2227 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 2228 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 2229 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 2230 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 2231 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 2232 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 2233 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 2234 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 2235 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 2236 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 2237 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 2238 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 2239 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 2240 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 2241 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 2242 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 2243 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 2244 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 2245 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 2246 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 2247 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 2248 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 2249 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 2250 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 2251 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 2252 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 2253 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 2254 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 2255 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 2256 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 2257 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 2258 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 2259 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 2260 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 2261 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 2262 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 2263 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 2264 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 2265 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 2266 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 2267 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 2268 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 2269 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 2270 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 2271 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 2272 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 2273 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 2274 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 2275 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 2276 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 2277 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 2278 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 2279 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 2280 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 2281 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 2282 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 2283 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 2284 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 2285 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 2286 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 2287 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 2288 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 2289 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 2290 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 2291 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 2292 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 2293 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 2294 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 2295 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 2296 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 2297 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 2298 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 2299 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 2300 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 2301 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 2302 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 2303 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 2304 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 2305 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 2306 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 2307 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 2308 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 2309 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 2310 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 2311 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 2312 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 2313 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 2314 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 2315 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 2316 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 2317 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 2318 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 2319 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 2320 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 2321 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 2322 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 2323 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 2324 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 2325 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 2326 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 2327 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 2328 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 2329 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 2330 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 2331 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 2332 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 2333 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 2334 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 2335 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 2336 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 2337 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 2338 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 2339 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 2340 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 2341 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 2342 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 2343 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 2344 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 2345 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 2346 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 2347 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 2348 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 2349 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 2350 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 2351 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 2352 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 2353 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 2354 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 2355 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 2356 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 2357 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 2358 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 2359 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 2360 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 2361 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 2362 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 2363 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 2364 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 2365 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 2366 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 2367 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 2368 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 2369 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 2370 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 2371 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 2372 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 2373 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 2374 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 2375 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 2376 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 2377 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 2378 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 2379 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 2380 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 2381 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 2382 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 2383 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 2384 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 2385 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 2386 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 2387 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 2388 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 2389 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 2390 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 2391 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 2392 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 2393 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 2394 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 2395 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 2396 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 2397 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 2398 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 2399 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 2400 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 2401 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 2402 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 2403 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 2404 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 2405 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 2406 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 2407 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 2408 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 2409 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 2410 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 2411 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 2412 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 2413 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 2414 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 2415 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 2416 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 2417 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 2418 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 2419 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 2420 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 2421 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 2422 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 2423 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 2424 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 2425 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 2426 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 2427 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 2428 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 2429 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 2430 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 2431 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 2432 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 2433 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 2434 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 2435 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 2436 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 2437 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 2438 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 2439 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 2440 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 2441 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 2442 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 2443 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 2444 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 2445 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 2446 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 2447 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 2448 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 2449 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 2450 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 2451 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 2452 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 2453 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 2454 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 2455 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 2456 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 2457 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 2458 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 2459 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 2460 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 2461 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 2462 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 2463 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 2464 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 2465 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 2466 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 2467 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 2468 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 2469 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 2470 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 2471 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 2472 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 2473 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 2474 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 2475 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 2476 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 2477 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 2478 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 2479 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 2480 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 2481 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 2482 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 2483 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 2484 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 2485 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 2486 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 2487 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 2488 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 2489 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 2490 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 2491 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 2492 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 2493 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 2494 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 2495 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 2496 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 2497 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 2498 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 2499 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 2500 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 2501 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 2502 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 2503 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 2504 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 2505 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 2506 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 2507 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 2508 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 2509 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 2510 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 2511 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 2512 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 2513 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 2514 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 2515 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 2516 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 2517 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 2518 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 2519 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 2520 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 2521 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 2522 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 2523 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 2524 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 2525 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 2526 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 2527 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 2528 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 2529 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 2530 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 2531 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 2532 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 2533 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 2534 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 2535 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 2536 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 2537 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 2538 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 2539 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 2540 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 2541 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 2542 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 2543 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 2544 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 2545 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 2546 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 2547 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 2548 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 2549 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 2550 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 2551 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 2552 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 2553 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 2554 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 2555 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 2556 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 2557 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 2558 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 2559 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 2560 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 2561 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 2562 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 2563 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 2564 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 2565 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 2566 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 2567 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 2568 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 2569 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 2570 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 2571 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 2572 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 2573 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 2574 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 2575 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 2576 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 2577 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 2578 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 2579 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 2580 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 2581 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 2582 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 2583 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 2584 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 2585 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 2586 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 2587 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 2588 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 2589 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 2590 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 2591 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 2592 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 2593 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 2594 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 2595 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 2596 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 2597 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 2598 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 2599 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 2600 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 2601 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 2602 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 2603 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 2604 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 2605 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 2606 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 2607 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 2608 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 2609 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 2610 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 2611 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 2612 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 2613 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 2614 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 2615 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 2616 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 2617 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 2618 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 2619 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 2620 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 2621 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 2622 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 2623 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 2624 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 2625 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 2626 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 2627 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 2628 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 2629 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 2630 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 2631 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 2632 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 2633 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 2634 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 2635 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 2636 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 2637 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 2638 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 2639 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 2640 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 2641 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 2642 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 2643 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 2644 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 2645 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 2646 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 2647 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 2648 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 2649 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 2650 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 2651 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 2652 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 2653 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 2654 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 2655 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 2656 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 2657 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 2658 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 2659 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 2660 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 2661 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 2662 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 2663 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 2664 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 2665 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 2666 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 2667 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 2668 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 2669 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 2670 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 2671 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 2672 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 2673 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 2674 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 2675 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 2676 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 2677 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 2678 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 2679 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 2680 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 2681 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 2682 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 2683 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 2684 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 2685 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 2686 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 2687 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 2688 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 2689 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 2690 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 2691 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 2692 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 2693 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 2694 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 2695 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 2696 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 2697 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 2698 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 2699 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 2700 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 2701 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 2702 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 2703 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 2704 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 2705 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 2706 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 2707 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 2708 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 2709 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 2710 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 2711 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 2712 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 2713 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 2714 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 2715 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 2716 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 2717 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 2718 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 2719 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 2720 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 2721 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 2722 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 2723 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 2724 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 2725 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 2726 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 2727 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 2728 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 2729 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 2730 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 2731 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 2732 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 2733 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 2734 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 2735 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 2736 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 2737 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 2738 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 2739 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 2740 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 2741 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 2742 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 2743 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 2744 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 2745 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 2746 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 2747 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 2748 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 2749 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 2750 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 2751 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 2752 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 2753 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 2754 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 2755 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 2756 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 2757 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 2758 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 2759 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 2760 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 2761 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 2762 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 2763 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 2764 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 2765 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 2766 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 2767 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 2768 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 2769 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 2770 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 2771 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 2772 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 2773 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 2774 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 2775 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 2776 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 2777 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 2778 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 2779 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 2780 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 2781 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 2782 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 2783 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 2784 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 2785 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 2786 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 2787 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 2788 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 2789 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 2790 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 2791 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 2792 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 2793 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 2794 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 2795 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 2796 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 2797 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 2798 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 2799 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 2800 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 2801 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 2802 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 2803 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 2804 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 2805 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 2806 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 2807 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 2808 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 2809 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 2810 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 2811 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 2812 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 2813 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 2814 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 2815 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 2816 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 2817 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 2818 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 2819 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 2820 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 2821 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 2822 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 2823 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 2824 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 2825 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 2826 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 2827 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 2828 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 2829 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 2830 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 2831 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 2832 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 2833 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 2834 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 2835 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 2836 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 2837 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 2838 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 2839 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 2840 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 2841 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 2842 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 2843 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 2844 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 2845 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 2846 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 2847 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 2848 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 2849 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 2850 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 2851 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 2852 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 2853 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 2854 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 2855 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 2856 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 2857 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 2858 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 2859 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 2860 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 2861 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 2862 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 2863 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 2864 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 2865 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 2866 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 2867 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 2868 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 2869 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 2870 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 2871 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 2872 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 2873 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 2874 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 2875 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 2876 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 2877 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 2878 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 2879 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 2880 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 2881 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 2882 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 2883 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 2884 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 2885 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 2886 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 2887 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 2888 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 2889 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 2890 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 2891 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 2892 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 2893 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 2894 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 2895 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 2896 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 2897 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 2898 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 2899 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 2900 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 2901 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 2902 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 2903 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 2904 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 2905 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 2906 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 2907 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 2908 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 2909 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 2910 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 2911 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 2912 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 2913 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 2914 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 2915 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 2916 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 2917 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 2918 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 2919 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 2920 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 2921 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 2922 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 2923 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 2924 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 2925 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 2926 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 2927 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 2928 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 2929 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 2930 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 2931 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 2932 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 2933 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 2934 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 2935 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 2936 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 2937 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 2938 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 2939 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 2940 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 2941 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 2942 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 2943 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 2944 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 2945 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 2946 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 2947 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 2948 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 2949 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 2950 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 2951 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 2952 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 2953 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 2954 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 2955 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 2956 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 2957 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 2958 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 2959 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 2960 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 2961 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 2962 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 2963 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 2964 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 2965 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 2966 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 2967 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 2968 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 2969 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 2970 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 2971 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 2972 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 2973 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 2974 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 2975 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 2976 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 2977 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 2978 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 2979 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 2980 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 2981 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 2982 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 2983 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 2984 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 2985 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 2986 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 2987 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 2988 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 2989 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 2990 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 2991 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 2992 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 2993 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 2994 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 2995 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 2996 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 2997 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 2998 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 2999 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 3000 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 3001 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 3002 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 3003 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 3004 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 3005 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 3006 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 3007 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 3008 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 3009 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 3010 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 3011 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 3012 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 3013 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 3014 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 3015 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 3016 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 3017 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 3018 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 3019 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 3020 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 3021 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 3022 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 3023 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 3024 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 3025 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 3026 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 3027 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 3028 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 3029 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 3030 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 3031 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 3032 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 3033 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 3034 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 3035 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 3036 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 3037 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 3038 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 3039 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 3040 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 3041 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 3042 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 3043 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 3044 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 3045 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 3046 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 3047 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 3048 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 3049 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 3050 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 3051 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 3052 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 3053 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 3054 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 3055 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 3056 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 3057 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 3058 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 3059 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 3060 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 3061 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 3062 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 3063 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 3064 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 3065 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 3066 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 3067 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 3068 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 3069 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 3070 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 3071 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 3072 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 3073 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 3074 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 3075 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 3076 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 3077 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 3078 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 3079 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 3080 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 3081 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 3082 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 3083 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 3084 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 3085 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 3086 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 3087 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 3088 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 3089 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 3090 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 3091 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 3092 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 3093 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 3094 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 3095 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 3096 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 3097 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 3098 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 3099 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 3100 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 3101 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 3102 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 3103 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 3104 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 3105 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 3106 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 3107 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 3108 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 3109 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 3110 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 3111 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 3112 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 3113 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 3114 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 3115 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 3116 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 3117 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 3118 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 3119 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 3120 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 3121 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 3122 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 3123 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 3124 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 3125 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 3126 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 3127 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 3128 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 3129 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 3130 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 3131 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 3132 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 3133 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 3134 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 3135 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 3136 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 3137 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 3138 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 3139 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 3140 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 3141 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 3142 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 3143 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 3144 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 3145 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 3146 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 3147 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 3148 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 3149 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 3150 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 3151 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 3152 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 3153 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 3154 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 3155 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 3156 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 3157 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 3158 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 3159 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 3160 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 3161 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 3162 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 3163 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 3164 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 3165 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 3166 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 3167 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 3168 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 3169 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 3170 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 3171 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 3172 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 3173 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 3174 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 3175 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 3176 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 3177 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 3178 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 3179 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 3180 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 3181 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 3182 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 3183 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 3184 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 3185 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 3186 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 3187 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 3188 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 3189 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 3190 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 3191 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 3192 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 3193 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 3194 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 3195 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 3196 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 3197 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 3198 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 3199 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 3200 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 3201 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 3202 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 3203 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 3204 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 3205 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 3206 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 3207 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 3208 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 3209 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 3210 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 3211 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 3212 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 3213 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 3214 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 3215 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 3216 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 3217 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 3218 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 3219 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 3220 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 3221 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 3222 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 3223 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 3224 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 3225 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 3226 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 3227 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 3228 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 3229 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 3230 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 3231 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 3232 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 3233 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 3234 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 3235 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 3236 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 3237 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 3238 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 3239 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 3240 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 3241 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 3242 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 3243 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 3244 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 3245 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 3246 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 3247 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 3248 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 3249 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 3250 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 3251 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 3252 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 3253 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 3254 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 3255 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 3256 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 3257 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 3258 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 3259 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 3260 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 3261 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 3262 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 3263 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 3264 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 3265 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 3266 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 3267 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 3268 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 3269 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 3270 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 3271 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 3272 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 3273 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 3274 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 3275 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 3276 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 3277 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 3278 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 3279 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 3280 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 3281 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 3282 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 3283 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 3284 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 3285 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 3286 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 3287 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 3288 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 3289 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 3290 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 3291 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 3292 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 3293 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 3294 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 3295 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 3296 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 3297 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 3298 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 3299 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 3300 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 3301 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 3302 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 3303 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 3304 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 3305 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 3306 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 3307 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 3308 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 3309 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 3310 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 3311 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 3312 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 3313 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 3314 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 3315 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 3316 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 3317 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 3318 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 3319 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 3320 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 3321 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 3322 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 3323 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 3324 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 3325 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 3326 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 3327 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 3328 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 3329 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 3330 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 3331 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 3332 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 3333 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 3334 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 3335 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 3336 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 3337 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 3338 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 3339 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 3340 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 3341 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 3342 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 3343 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 3344 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 3345 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 3346 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 3347 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 3348 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 3349 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 3350 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 3351 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 3352 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 3353 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 3354 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 3355 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 3356 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 3357 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 3358 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 3359 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 3360 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 3361 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 3362 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 3363 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 3364 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 3365 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 3366 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 3367 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 3368 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 3369 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 3370 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 3371 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 3372 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 3373 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 3374 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 3375 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 3376 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 3377 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 3378 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 3379 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 3380 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 3381 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 3382 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 3383 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 3384 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 3385 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 3386 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 3387 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 3388 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 3389 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 3390 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 3391 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 3392 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 3393 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 3394 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 3395 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 3396 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 3397 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 3398 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 3399 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 3400 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 3401 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 3402 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 3403 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 3404 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 3405 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 3406 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 3407 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 3408 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 3409 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 3410 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 3411 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 3412 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 3413 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 3414 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 3415 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 3416 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 3417 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 3418 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 3419 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 3420 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 3421 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 3422 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 3423 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 3424 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 3425 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 3426 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 3427 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 3428 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 3429 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 3430 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 3431 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 3432 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 3433 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 3434 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 3435 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 3436 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 3437 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 3438 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 3439 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 3440 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 3441 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 3442 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 3443 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 3444 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 3445 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 3446 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 3447 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 3448 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 3449 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 3450 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 3451 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 3452 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 3453 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 3454 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 3455 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 3456 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 3457 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 3458 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 3459 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 3460 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 3461 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 3462 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 3463 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 3464 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 3465 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 3466 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 3467 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 3468 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 3469 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 3470 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 3471 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 3472 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 3473 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 3474 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 3475 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 3476 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 3477 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 3478 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 3479 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 3480 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 3481 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 3482 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 3483 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 3484 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 3485 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 3486 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 3487 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 3488 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 3489 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 3490 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 3491 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 3492 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 3493 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 3494 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 3495 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 3496 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 3497 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 3498 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 3499 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 3500 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 3501 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 3502 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 3503 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 3504 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 3505 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 3506 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 3507 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 3508 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 3509 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 3510 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 3511 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 3512 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 3513 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 3514 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 3515 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 3516 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 3517 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 3518 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 3519 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 3520 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 3521 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 3522 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 3523 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 3524 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 3525 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 3526 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 3527 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 3528 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 3529 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 3530 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 3531 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 3532 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 3533 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 3534 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 3535 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 3536 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 3537 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 3538 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 3539 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 3540 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 3541 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 3542 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 3543 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 3544 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 3545 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 3546 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 3547 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 3548 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 3549 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 3550 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 3551 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 3552 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 3553 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 3554 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 3555 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 3556 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 3557 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 3558 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 3559 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 3560 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 3561 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 3562 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 3563 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 3564 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 3565 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 3566 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 3567 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 3568 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 3569 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 3570 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 3571 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 3572 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 3573 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 3574 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 3575 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 3576 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 3577 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 3578 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 3579 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 3580 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 3581 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 3582 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 3583 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 3584 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 3585 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 3586 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 3587 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 3588 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 3589 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 3590 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 3591 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 3592 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 3593 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 3594 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 3595 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 3596 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 3597 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 3598 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 3599 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 3600 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 3601 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 3602 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 3603 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 3604 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 3605 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 3606 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 3607 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 3608 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 3609 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 3610 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 3611 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 3612 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 3613 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 3614 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 3615 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 3616 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 3617 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 3618 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 3619 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 3620 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 3621 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 3622 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 3623 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 3624 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 3625 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 3626 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 3627 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 3628 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 3629 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 3630 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 3631 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 3632 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 3633 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 3634 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 3635 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 3636 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 3637 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 3638 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 3639 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 3640 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 3641 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 3642 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 3643 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 3644 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 3645 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 3646 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 3647 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 3648 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 3649 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 3650 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 3651 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 3652 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 3653 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 3654 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 3655 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 3656 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 3657 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 3658 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 3659 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 3660 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 3661 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 3662 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 3663 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 3664 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 3665 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 3666 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 3667 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 3668 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 3669 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 3670 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 3671 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 3672 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 3673 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 3674 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 3675 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 3676 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 3677 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 3678 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 3679 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 3680 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 3681 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 3682 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 3683 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 3684 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 3685 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 3686 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 3687 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 3688 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 3689 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 3690 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 3691 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 3692 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 3693 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 3694 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 3695 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 3696 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 3697 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 3698 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 3699 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 3700 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 3701 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 3702 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 3703 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 3704 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 3705 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 3706 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 3707 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 3708 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 3709 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 3710 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 3711 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 3712 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 3713 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 3714 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 3715 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 3716 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 3717 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 3718 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 3719 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 3720 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 3721 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 3722 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 3723 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 3724 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 3725 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 3726 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 3727 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 3728 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 3729 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 3730 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 3731 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 3732 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 3733 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 3734 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 3735 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 3736 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 3737 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 3738 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 3739 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 3740 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 3741 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 3742 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 3743 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 3744 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 3745 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 3746 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 3747 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 3748 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 3749 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 3750 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 3751 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 3752 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 3753 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 3754 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 3755 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 3756 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 3757 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 3758 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 3759 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 3760 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 3761 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 3762 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 3763 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 3764 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 3765 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 3766 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 3767 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 3768 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 3769 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 3770 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 3771 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 3772 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 3773 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 3774 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 3775 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 3776 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 3777 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 3778 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 3779 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 3780 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 3781 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 3782 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 3783 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 3784 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 3785 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 3786 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 3787 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 3788 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 3789 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 3790 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 3791 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 3792 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 3793 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 3794 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 3795 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 3796 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 3797 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 3798 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 3799 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 3800 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 3801 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 3802 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 3803 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 3804 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 3805 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 3806 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 3807 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 3808 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 3809 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 3810 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 3811 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 3812 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 3813 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 3814 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 3815 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 3816 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 3817 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 3818 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 3819 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 3820 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 3821 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 3822 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 3823 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 3824 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 3825 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 3826 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 3827 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 3828 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 3829 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 3830 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 3831 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 3832 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 3833 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 3834 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 3835 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 3836 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 3837 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 3838 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 3839 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 3840 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 3841 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 3842 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 3843 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 3844 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 3845 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 3846 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 3847 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 3848 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 3849 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 3850 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 3851 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 3852 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 3853 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 3854 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 3855 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 3856 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 3857 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 3858 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 3859 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 3860 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 3861 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 3862 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 3863 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 3864 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 3865 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 3866 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 3867 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 3868 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 3869 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 3870 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 3871 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 3872 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 3873 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 3874 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 3875 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 3876 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 3877 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 3878 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 3879 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 3880 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 3881 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 3882 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 3883 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 3884 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 3885 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 3886 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 3887 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 3888 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 3889 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 3890 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 3891 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 3892 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 3893 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 3894 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 3895 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 3896 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 3897 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 3898 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 3899 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 3900 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 3901 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 3902 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 3903 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 3904 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 3905 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 3906 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 3907 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 3908 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 3909 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 3910 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 3911 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 3912 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 3913 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 3914 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 3915 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 3916 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 3917 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 3918 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 3919 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 3920 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 3921 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 3922 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 3923 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 3924 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 3925 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 3926 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 3927 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 3928 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 3929 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 3930 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 3931 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 3932 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 3933 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 3934 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 3935 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 3936 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 3937 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 3938 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 3939 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 3940 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 3941 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 3942 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 3943 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 3944 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 3945 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 3946 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 3947 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 3948 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 3949 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 3950 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 3951 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 3952 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 3953 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 3954 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 3955 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 3956 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 3957 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 3958 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 3959 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 3960 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 3961 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 3962 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 3963 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 3964 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 3965 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 3966 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 3967 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 3968 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 3969 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 3970 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 3971 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 3972 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 3973 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 3974 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 3975 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 3976 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 3977 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 3978 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 3979 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 3980 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 3981 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 3982 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 3983 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 3984 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 3985 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 3986 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 3987 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 3988 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 3989 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 3990 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 3991 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 3992 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 3993 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 3994 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 3995 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 3996 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 3997 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 3998 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 3999 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 4000 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 4001 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 4002 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 4003 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 4004 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 4005 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 4006 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 4007 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 4008 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 4009 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 4010 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 4011 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 4012 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 4013 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 4014 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 4015 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 4016 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 4017 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 4018 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 4019 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 4020 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 4021 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 4022 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 4023 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 4024 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 4025 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 4026 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 4027 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 4028 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 4029 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 4030 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 4031 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 4032 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 4033 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 4034 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 4035 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 4036 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 4037 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 4038 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 4039 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 4040 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 4041 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 4042 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 4043 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 4044 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 4045 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 4046 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 4047 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 4048 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 4049 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 4050 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 4051 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 4052 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 4053 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 4054 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 4055 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 4056 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 4057 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 4058 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 4059 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 4060 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 4061 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 4062 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 4063 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 4064 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 4065 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 4066 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 4067 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 4068 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 4069 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 4070 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 4071 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 4072 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 4073 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 4074 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 4075 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 4076 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 4077 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 4078 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 4079 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 4080 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 4081 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 4082 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 4083 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 4084 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 4085 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 4086 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 4087 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 4088 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 4089 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 4090 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 4091 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 4092 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 4093 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 4094 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 4095 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 4096 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 4097 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 4098 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 4099 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 4100 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 4101 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 4102 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 4103 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 4104 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 4105 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 4106 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 4107 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 4108 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 4109 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 4110 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 4111 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 4112 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 4113 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 4114 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 4115 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 4116 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 4117 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 4118 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 4119 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 4120 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 4121 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 4122 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 4123 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 4124 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 4125 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 4126 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 4127 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 4128 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 4129 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 4130 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 4131 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 4132 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 4133 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 4134 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 4135 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 4136 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 4137 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 4138 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 4139 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 4140 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 4141 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 4142 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 4143 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 4144 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 4145 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 4146 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 4147 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 4148 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 4149 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 4150 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 4151 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 4152 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 4153 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 4154 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 4155 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 4156 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 4157 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 4158 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 4159 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 4160 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 4161 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 4162 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 4163 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 4164 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 4165 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 4166 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 4167 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 4168 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 4169 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 4170 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 4171 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 4172 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 4173 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 4174 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 4175 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 4176 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 4177 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 4178 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 4179 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 4180 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 4181 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 4182 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 4183 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 4184 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 4185 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 4186 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 4187 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 4188 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 4189 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 4190 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 4191 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 4192 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 4193 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 4194 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 4195 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 4196 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 4197 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 4198 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 4199 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 4200 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 4201 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 4202 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 4203 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 4204 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 4205 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 4206 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 4207 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 4208 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 4209 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 4210 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 4211 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 4212 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 4213 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 4214 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 4215 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 4216 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 4217 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 4218 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 4219 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 4220 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 4221 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 4222 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 4223 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 4224 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 4225 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 4226 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 4227 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 4228 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 4229 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 4230 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 4231 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 4232 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 4233 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 4234 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 4235 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 4236 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 4237 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 4238 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 4239 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 4240 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 4241 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 4242 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 4243 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 4244 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 4245 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 4246 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 4247 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 4248 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 4249 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 4250 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 4251 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 4252 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 4253 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 4254 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 4255 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 4256 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 4257 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 4258 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 4259 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 4260 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 4261 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 4262 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 4263 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 4264 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 4265 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 4266 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 4267 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 4268 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 4269 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 4270 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 4271 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 4272 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 4273 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 4274 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 4275 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 4276 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 4277 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 4278 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 4279 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 4280 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 4281 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 4282 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 4283 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 4284 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 4285 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 4286 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 4287 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 4288 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 4289 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 4290 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 4291 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 4292 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 4293 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 4294 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 4295 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 4296 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 4297 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 4298 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 4299 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 4300 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 4301 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 4302 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 4303 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 4304 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 4305 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 4306 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 4307 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 4308 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 4309 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 4310 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 4311 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 4312 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 4313 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 4314 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 4315 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 4316 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 4317 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 4318 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 4319 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 4320 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 4321 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 4322 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 4323 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 4324 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 4325 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 4326 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 4327 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 4328 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 4329 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 4330 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 4331 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 4332 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 4333 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 4334 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 4335 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 4336 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 4337 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 4338 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 4339 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 4340 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 4341 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 4342 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 4343 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 4344 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 4345 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 4346 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 4347 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 4348 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 4349 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 4350 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 4351 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 4352 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 4353 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 4354 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 4355 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 4356 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 4357 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 4358 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 4359 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 4360 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 4361 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 4362 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 4363 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 4364 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 4365 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 4366 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 4367 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 4368 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 4369 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 4370 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 4371 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 4372 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 4373 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 4374 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 4375 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 4376 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 4377 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 4378 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 4379 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 4380 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 4381 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 4382 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 4383 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 4384 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 4385 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 4386 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 4387 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 4388 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 4389 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 4390 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 4391 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 4392 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 4393 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 4394 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 4395 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 4396 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 4397 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 4398 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 4399 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 4400 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 4401 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 4402 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 4403 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 4404 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 4405 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 4406 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 4407 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 4408 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 4409 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 4410 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 4411 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 4412 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 4413 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 4414 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 4415 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 4416 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 4417 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 4418 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 4419 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 4420 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 4421 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 4422 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 4423 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 4424 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 4425 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 4426 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 4427 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 4428 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 4429 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 4430 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 4431 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 4432 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 4433 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 4434 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 4435 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 4436 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 4437 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 4438 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 4439 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 4440 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 4441 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 4442 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 4443 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 4444 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 4445 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 4446 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 4447 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 4448 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 4449 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 4450 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 4451 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 4452 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 4453 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 4454 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 4455 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 4456 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 4457 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 4458 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 4459 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 4460 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 4461 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 4462 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 4463 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 4464 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 4465 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 4466 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 4467 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 4468 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 4469 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 4470 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 4471 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 4472 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 4473 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 4474 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 4475 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 4476 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 4477 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 4478 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 4479 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 4480 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 4481 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 4482 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 4483 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 4484 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 4485 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 4486 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 4487 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 4488 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 4489 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 4490 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 4491 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 4492 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 4493 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 4494 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 4495 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 4496 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 4497 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 4498 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 4499 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 4500 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 4501 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 4502 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 4503 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 4504 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 4505 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 4506 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 4507 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 4508 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 4509 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 4510 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 4511 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 4512 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 4513 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 4514 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 4515 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 4516 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 4517 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 4518 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 4519 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 4520 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 4521 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 4522 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 4523 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 4524 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 4525 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 4526 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 4527 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 4528 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 4529 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 4530 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 4531 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 4532 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 4533 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 4534 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 4535 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 4536 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 4537 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 4538 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 4539 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 4540 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 4541 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 4542 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 4543 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 4544 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 4545 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 4546 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 4547 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 4548 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 4549 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 4550 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 4551 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 4552 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 4553 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 4554 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 4555 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 4556 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 4557 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 4558 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 4559 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 4560 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 4561 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 4562 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 4563 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 4564 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 4565 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 4566 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 4567 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 4568 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 4569 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 4570 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 4571 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 4572 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 4573 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 4574 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 4575 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 4576 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 4577 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 4578 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 4579 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 4580 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 4581 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 4582 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 4583 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 4584 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 4585 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 4586 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 4587 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 4588 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 4589 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 4590 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 4591 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 4592 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 4593 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 4594 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 4595 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 4596 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 4597 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 4598 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 4599 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 4600 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 4601 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 4602 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 4603 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 4604 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 4605 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 4606 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 4607 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 4608 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 4609 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 4610 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 4611 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 4612 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 4613 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 4614 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 4615 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 4616 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 4617 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 4618 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 4619 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 4620 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 4621 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 4622 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 4623 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 4624 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 4625 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 4626 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 4627 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 4628 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 4629 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 4630 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 4631 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 4632 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 4633 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 4634 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 4635 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 4636 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 4637 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 4638 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 4639 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 4640 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 4641 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 4642 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 4643 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 4644 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 4645 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 4646 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 4647 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 4648 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 4649 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 4650 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 4651 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 4652 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 4653 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 4654 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 4655 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 4656 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 4657 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 4658 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 4659 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 4660 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 4661 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 4662 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 4663 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 4664 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 4665 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 4666 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 4667 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 4668 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 4669 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 4670 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 4671 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 4672 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 4673 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 4674 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 4675 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 4676 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 4677 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 4678 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 4679 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 4680 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 4681 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 4682 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 4683 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 4684 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 4685 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 4686 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 4687 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 4688 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 4689 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 4690 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 4691 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 4692 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 4693 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 4694 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 4695 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 4696 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 4697 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 4698 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 4699 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 4700 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 4701 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 4702 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 4703 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 4704 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 4705 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 4706 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 4707 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 4708 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 4709 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 4710 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 4711 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 4712 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 4713 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 4714 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 4715 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 4716 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 4717 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 4718 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 4719 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 4720 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 4721 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 4722 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 4723 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 4724 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 4725 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 4726 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 4727 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 4728 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 4729 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 4730 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 4731 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 4732 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 4733 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 4734 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 4735 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 4736 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 4737 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 4738 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 4739 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 4740 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 4741 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 4742 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 4743 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 4744 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 4745 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 4746 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 4747 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 4748 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 4749 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 4750 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 4751 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 4752 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 4753 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 4754 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 4755 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 4756 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 4757 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 4758 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 4759 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 4760 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 4761 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 4762 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 4763 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 4764 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 4765 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 4766 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 4767 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 4768 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 4769 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 4770 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 4771 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 4772 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 4773 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 4774 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 4775 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 4776 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 4777 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 4778 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 4779 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 4780 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 4781 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 4782 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 4783 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 4784 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 4785 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 4786 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 4787 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 4788 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 4789 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 4790 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 4791 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 4792 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 4793 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 4794 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 4795 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 4796 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 4797 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 4798 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 4799 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 4800 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 4801 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 4802 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 4803 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 4804 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 4805 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 4806 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 4807 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 4808 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 4809 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 4810 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 4811 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 4812 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 4813 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 4814 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 4815 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 4816 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 4817 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 4818 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 4819 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 4820 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 4821 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 4822 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 4823 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 4824 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 4825 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 4826 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 4827 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 4828 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 4829 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 4830 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 4831 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 4832 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 4833 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 4834 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 4835 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 4836 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 4837 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 4838 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 4839 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 4840 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 4841 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 4842 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 4843 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 4844 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 4845 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 4846 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 4847 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 4848 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 4849 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 4850 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 4851 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 4852 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 4853 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 4854 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 4855 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 4856 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 4857 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 4858 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 4859 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 4860 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 4861 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 4862 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 4863 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 4864 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 4865 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 4866 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 4867 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 4868 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 4869 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 4870 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 4871 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 4872 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 4873 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 4874 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 4875 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 4876 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 4877 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 4878 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 4879 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 4880 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 4881 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 4882 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 4883 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 4884 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 4885 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 4886 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 4887 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 4888 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 4889 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 4890 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 4891 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 4892 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 4893 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 4894 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 4895 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 4896 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 4897 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 4898 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 4899 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 4900 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 4901 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 4902 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 4903 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 4904 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 4905 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 4906 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 4907 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 4908 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 4909 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 4910 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 4911 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 4912 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 4913 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 4914 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 4915 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 4916 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 4917 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 4918 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 4919 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 4920 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 4921 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 4922 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 4923 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 4924 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 4925 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 4926 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 4927 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 4928 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 4929 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 4930 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 4931 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 4932 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 4933 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 4934 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 4935 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 4936 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 4937 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 4938 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 4939 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 4940 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 4941 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 4942 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 4943 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 4944 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 4945 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 4946 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 4947 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 4948 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 4949 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 4950 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 4951 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 4952 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 4953 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 4954 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 4955 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 4956 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 4957 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 4958 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 4959 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 4960 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 4961 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 4962 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 4963 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 4964 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 4965 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 4966 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 4967 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 4968 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 4969 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 4970 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 4971 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 4972 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 4973 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 4974 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 4975 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 4976 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 4977 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 4978 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 4979 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 4980 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 4981 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 4982 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 4983 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 4984 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 4985 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 4986 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 4987 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 4988 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 4989 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 4990 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 4991 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 4992 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 4993 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n",
      "Epoch: 4994 | Train loss: 1.003399484034162e-05 | Test loss: 2.8780102638847893e-06\n",
      "Epoch: 4995 | Train loss: 2.7329101612849627e-06 | Test loss: 1.8427668692311272e-05\n",
      "Epoch: 4996 | Train loss: 1.5445519238710403e-05 | Test loss: 2.404212864348665e-05\n",
      "Epoch: 4997 | Train loss: 2.022445187321864e-05 | Test loss: 1.548707405163441e-05\n",
      "Epoch: 4998 | Train loss: 1.292141132580582e-05 | Test loss: 5.815326858282788e-06\n",
      "Epoch: 4999 | Train loss: 5.254745246929815e-06 | Test loss: 1.14315744212945e-05\n"
     ]
    }
   ],
   "source": [
    "epochs = 5000\n",
    "\n",
    "X_train = X_train.to(device)\n",
    "X_test = X_test.to(device)\n",
    "y_train = y_train.to(device)\n",
    "y_test = y_test.to(device)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    y_pred = model(X_train)\n",
    "\n",
    "    loss = loss_fn(y_pred, y_train)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        test_pred = model(X_test)\n",
    "\n",
    "        test_loss = loss_fn(test_pred, y_test)\n",
    "\n",
    "        print(f\"Epoch: {epoch} | Train loss: {loss} | Test loss: {test_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('linear_layer.weight', tensor([[0.9000]], device='cuda:0')),\n",
       "             ('linear_layer.bias', tensor([0.2000], device='cuda:0'))])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.state_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAzoAAAJGCAYAAACTJvC6AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8pXeV/AAAACXBIWXMAAA9hAAAPYQGoP6dpAABTZ0lEQVR4nO3deXxU5d3///dkZ0siWwgQCWJUrAgKBhNECEaxepNgcVywGKzVr4pWIZZCogSURWxEqgL2piLaVqVVkKn4QwsmWCUsBbGiAncEFKMJxCVBkASS8/sjnZEhs4Zk1tfz8chDOefMmWvCAfP2c12fy2QYhiEAAAAACCER/h4AAAAAALQ2gg4AAACAkEPQAQAAABByCDoAAAAAQg5BBwAAAEDIIegAAAAACDkEHQAAAAAhJ8rfA/BEY2OjvvrqK3Xq1Ekmk8nfwwEAAADgJ4Zh6PDhw+rZs6ciIpzXbYIi6Hz11VdKSUnx9zAAAAAABIgDBw6od+/eTs8HRdDp1KmTpKYPEx8f7+fRAAAAAPCX2tpapaSk2DKCM0ERdKzT1eLj4wk6AAAAANwuaaEZAQAAAICQQ9ABAAAAEHIIOgAAAABCDkEHAAAAQMgh6AAAAAAIOQQdAAAAACEnKNpLt8Tx48fV0NDg72EAfhEdHa3IyEh/DwMAAMBvQi7o1NbWqrq6WnV1df4eCuA3JpNJCQkJ6tGjh9se8wAAAKEopIJObW2tKioq1LFjR3Xt2lXR0dH8kIewYxiGjhw5okOHDqldu3ZKTEz095AAAAB8LqSCTnV1tTp27KjevXsTcBDW2rVrp7q6Oh08eFAJCQn8eQAAAGEnZJoRHD9+XHV1dfxQB/xXfHy8GhoaWKsGAADCUsgEHesPc9HR0X4eCRAYoqKaCrYnTpzw80gAAAB8L2SCjhXVHKAJfxYAAEA4C7mgAwAAAAAEHQAAAAAhh6CD02YymTRy5MjTukdpaalMJpNmzpzZKmNqa6mpqUpNTfX3MAAAAOAEQSdEmEwmr77gfyNHjuT3AgAAoI2E1D464ayoqKjZsYULF6qmpsbhudb06aefqn379qd1j/T0dH366afq2rVrK40KAAAA4YygEyIcTflavny5ampq2nw62HnnnXfa92jfvn2r3AcAAACQmLoWdvbv3y+TyaSJEyfq008/1XXXXacuXbrIZDJp//79kqRVq1bp5ptv1tlnn6327dsrISFBw4cP12uvvebwno7W6EycOFEmk0n79u3TU089pfPOO0+xsbHq06ePZs2apcbGRrvrna3Rsa6F+eGHH3T//ferZ8+eio2N1YUXXqhXX33V6We88cYb1blzZ3Xs2FEjRozQu+++q5kzZ8pkMqm0tNTj79fq1at1ySWXqF27dkpKStIdd9yh7777zuG1e/bs0dSpU3XxxRerS5cuiouL0znnnKNp06bphx9+aPY927Bhg+3frV8TJ060XbNs2TLl5uYqNTVVcXFx6ty5s0aPHq2SkhKPxw8AABCuqOiEqfLycl166aUaMGCAJk6cqG+++UYxMTGSpOnTpysmJkaXXXaZkpOTdejQIVksFl1//fV66qmndN9993n8Pr/97W+1YcMG/c///I9Gjx6t119/XTNnzlR9fb3mzJnj0T2OHz+uq666St99953GjRuno0eP6pVXXtENN9ygtWvX6qqrrrJdW1FRoczMTH399de6+uqrddFFF2n37t268sorNWrUKK++Ry+++KLy8vIUHx+vCRMmKDExUW+88Yays7NVX19v+35ZrVy5Us8995yysrI0cuRINTY2atOmTZo/f742bNigd99917ahbVFRkZYvX67PP//cbmrhoEGDbP8+adIkDRw4UNnZ2erWrZsqKir0+uuvKzs7WytXrlRubq5XnwcAAKAlSm4Zph7/+kCVwy9S1l/f9/dwPGcEgZqaGkOSUVNT4/SaH3/80fjkk0+MH3/80YcjC2x9+vQxTv0t3rdvnyHJkGTMmDHD4es+++yzZscOHz5sDBgwwEhISDCOHDlid06SMWLECLtjeXl5hiSjb9++xldffWU7fujQISMxMdHo1KmTUVdXZzteUlJiSDKKioocfobc3Fy769etW2dIMkaPHm13/S9/+UtDkjFnzhy7488995ztc5eUlDj83Cerqakx4uPjjQ4dOhi7d++2Ha+vrzcuv/xyQ5LRp08fu9d8+eWXdmO0mjVrliHJ+Mtf/mJ3fMSIEc1+f062d+/eZse++uoro2fPnkZaWprbz8CfCQAAcDo2LS4wPusaZRiS0SgZhmS8Mz7T38PyKBsYhmEwdS1M9ejRQ4WFhQ7PnXXWWc2OdezYURMnTlRNTY22bt3q8fs8/PDDSk5Otv26a9euys3N1eHDh7V7926P7/Pkk0/aVVCuuOIK9enTx24sdXV1+vvf/67u3bsrPz/f7vW33Xabzj33XI/f7/XXX1dtba1+9atf6ZxzzrEdj46OdlqJ6tWrV7MqjyTde++9kqR169Z5/P6S1Ldv32bHkpOTNW7cOP3f//2fPv/8c6/uBwAA4KmSW4Zp6D1zdVb1CUmSSU3/x7jHvz7w67i8QdBpIYtFmjy56Z/BaODAgQ5/KJekgwcPasqUKerfv7/at29vWz9iDQ9fffWVx+8zePDgZsd69+4tSfr+++89ukdiYqLDH/p79+5td4/du3errq5OQ4YMUWxsrN21JpNJmZmZHo/7ww8/lCQNHz682bmMjAxFRTWf9WkYhpYtW6bLL79cnTt3VmRkpEwmk7p06SLJu++bJO3du1d33HGH+vXrp7i4ONvvw9NPP92i+wEAAHhiY0ZvZb200e6YoaawUzn8Ir+MqSVYo9MCFouUmytFRkoLF0qrV0s5Of4elXeSkpIcHv/22291ySWX6IsvvtCwYcOUnZ2txMRERUZGaseOHVq9erXq6uo8fp/4+Phmx6whoaGhwaN7JCQkODweFRVl19SgtrZWktS9e3eH1zv7zI7U1NQ4vVdkZKQtvJzsN7/5jZ555hmlpKQoJydHycnJtsA1a9Ysr75v5eXlSk9PV21trbKysjRmzBjFx8crIiJCpaWl2rBhg1f3AwAAcGfzkkJ1m/G4MqtP2IKNlUnSxkt7BdUaHYJOC5SUNIWchoamf5aWBl/QcbZR5XPPPacvvvhCjz76qB566CG7c4899phWr17ti+G1iDVUHTx40OH5qqoqj+9lDVeO7tXQ0KBvvvlGvXr1sh07ePCgFi1apAsvvFBlZWV2+wpVVlZq1qxZHr+31DRV77vvvtOf//xn/fKXv7Q7d9ddd9k6tgEAALSGjRm9lbmpwvbrU39SLBmfGVQhR2LqWotkZf0UchoapFM6Kwe1zz77TJIcdvT617/+5evheOXcc89VbGystm3b1qzaYRiGysrKPL7XwIEDJTn+zGVlZTpx4oTdsb1798owDGVnZzfbPNXZ9y0yMlKS48qWs98HwzD0/vvB9ZcMAAAIXJuXFGpvt2hlbqqQcdJx67/v6xalzYsLgi7kSASdFsnJaZqu9pvfBOe0NVf69OkjSXrvvffsjr/00kt68803/TEkj8XGxur6669XVVWVFi5caHfuxRdf1K5duzy+V25uruLj47Vs2TLt2bPHdvz48ePNKl3ST9+3jRs32k2n+/LLLzV9+nSH79G5c2dJ0oEDB5ze79Tfh8cee0w7d+70+HMAAAA446jhgJV1qlrfg8c19G7PtgQJNExda6GcnNAKOFYTJkzQ/Pnzdd9996mkpER9+vTRhx9+qPXr1+sXv/iFVq5c6e8hujRv3jytW7dO06ZN04YNG2z76Lzxxhu6+uqrtXbtWkVEuM/3CQkJeuqppzRx4kRdcskluummm5SQkKA33nhD7dq1s+skJ/3UDe21117TkCFDdMUVV6iqqkpvvPGGrrjiCluF5mSjRo3Sq6++qnHjxunnP/+54uLiNHDgQI0ZM0Z33XWXnn/+eY0bN0433HCDunTpok2bNmn79u269tprtWbNmlb7ngEAgPCzMaO3sk6aqib91HBgX7coHZw1VZlBGnCsqOjATu/evbVhwwZdccUVWrdunf74xz+qvr5eb7/9tsaMGePv4bmVkpKisrIymc1mbdy4UQsXLtTBgwf19ttv6+yzz5bkuEGCI3l5eVq1apXS0tL0wgsv6IUXXtCwYcO0bt06hx3rli9frvz8fH333Xd6+umntWnTJk2ZMkUvvfSSw/vfcccdmjp1qqqrqzV//nw9/PDDeu211yRJF110kd5++21dfPHFWrlypZYtW6bExES9//77GjJkSAu/OwAAINw5m6omhUYV52QmwzBO/YwBp7a2VgkJCaqpqXH6Q+qxY8e0b98+9e3bV3FxcT4eIYLBZZddprKyMtXU1Khjx47+Hk6b488EAAA42akNB04VLA0HPMkGEhUdhKCvv/662bG//OUvev/995WdnR0WIQcAAMAqlBsOuMIaHYScCy64QBdddJHOP/982/4/paWl6tSpk4qLi/09PAAAAJ8puWWY3eafjhoOZJZ9qeZbswc/KjoIOXfddZcOHjyoF198Uc8884x2796t8ePHa8uWLRowYIC/hwcAANDmNi8p1M7UDnYhR2pexcks+9L3g/MRKjoIOXPmzNGcOcG/gA4AAMBbm5cUqsP8hRr6+VGH50O9inMygg4AAAAQAk6dpubwmiBpONAaCDoAAABAkHO0L87JdqZ20JGp9ysrBNpGe4qgAwAAAASpzUsK1W3G48qsPmHb8PNU4VTFORlBBwAAAAhCp+6Lc2rICccqzsnougYAAAAEEU/3xblg3w8aGqYhR6KiAwAAAASNcN4Xx1tUdAAAAIAAx7443qOiAwAAAAQwV22jqeI4R0UHAAAACFAbM3q73BunZHwmVRwnCDrwiZEjR8pkctTwMPAsX75cJpNJy5cv9/dQAABAmHLWcMBqZ2oHbV5cEJZtoz1F0AkRJpPJq6/WNnPmTJlMJpWWlrb6vYNRaWmpTCaTZs6c6e+hAACAIFNyyzANvWeuzqo+Ial52+iS8Zlh31HNE6zRCRFFRUXNji1cuFA1NTUOz/naiy++qKNHj/p7GAAAAAFr85JCdZi/UFmf2//MZN0IdF+3KB2cNTVs98XxFkEnRDiqHCxfvlw1NTUBUVU488wz/T0EAACAgEXDgdbH1LUwVF9frwULFujiiy9Whw4d1KlTJw0fPlwWi6XZtTU1NZoxY4bOP/98dezYUfHx8Tr77LOVl5enzz//XFLT+ptZs2ZJkrKysmzT41JTU233cbRG5+S1MG+//bYyMzPVvn17denSRXl5efrmm28cjv+Pf/yjfvaznykuLk4pKSmaOnWqjh07JpPJpJEjR3r8ffj222911113KSkpSe3bt9cll1yiVatWOb1+2bJlys3NVWpqquLi4tS5c2eNHj1aJSUldtfNnDlTWVlZkqRZs2bZTRncv3+/JGnPnj2aOnWqLr74YnXp0kVxcXE655xzNG3aNP3www8efwYAABD8aDjQNqjohJm6ujpdffXVKi0t1aBBg3T77bfr+PHjWrNmjXJzc/X000/r3nvvlSQZhqHRo0dr8+bNGjZsmK6++mpFRETo888/l8Vi0YQJE9SnTx9NnDhRkrRhwwbl5eXZAk5iYqJHY7JYLFqzZo3GjBmjzMxMvfvuu3rxxRf12Wef6b333rO7dsaMGXr00UeVlJSkO+64Q9HR0frb3/6mXbt2efV9OHr0qEaOHKmPPvpIGRkZGjFihA4cOKAbb7xRV111lcPXTJo0SQMHDlR2dra6deumiooKvf7668rOztbKlSuVm5srqSnU7d+/Xy+88IJGjBhhF76s35OVK1fqueeeU1ZWlkaOHKnGxkZt2rRJ8+fP14YNG/Tuu+8qOjraq88EAACCy+Ylheo243FlVp+wTU872c7UDjoy9X6mqrWUEQRqamoMSUZNTY3Ta3788Ufjk08+MX788Ucfjiyw9enTxzj1t7igoMCQZDz88MNGY2Oj7Xhtba0xZMgQIyYmxqioqDAMwzD+85//GJKMsWPHNrv3sWPHjMOHD9t+XVRUZEgySkpKHI5lxIgRzcby/PPPG5KMqKgo47333rMdP3HihDFy5EhDklFWVmY7vnv3biMyMtLo1auXUVVVZTf2888/35BkjBgxwv035qTx3nHHHXbH165da6hpKqzx/PPP253bu3dvs/t89dVXRs+ePY20tDS74yUlJYYko6ioyOH7f/nll0ZdXV2z47NmzTIkGX/5y188+hyu8GcCAIDA9f6lvQxDcvr1zvhMfw8xYHmSDQzDMJi61kKW3RZNXjtZlt3Np3sFqsbGRi1ZskT9+vWzTamy6tSpk2bMmKH6+nqtXLnS7nXt2rVrdq/Y2Fh17NixVcY1fvx4DRs2zPbryMhI5eXlSZK2bt1qO/7yyy+roaFB+fn56t69u93YH3roIa/e88UXX1RMTIweeeQRu+OjR4/WFVdc4fA1ffs2nxWbnJyscePG6f/+7/9sU/k80atXL8XExDQ7bq2mrVu3zuN7AQCA4OGsbbT13/d1i6JtdCth6loLWHZblPtKriJNkVq4eaFW37RaOefm+HtYbu3evVvfffedevbsaVtTc7JDhw5Jkm0aWP/+/XXhhRfq5Zdf1pdffqmxY8dq5MiRGjRokCIiWi8jDx48uNmx3r17S5K+//5727EPP/xQknTZZZc1u/7koORObW2t9u3bp/PPP189evRodn748OFav359s+N79+7VvHnz9M4776iiokJ1dXV257/66iv16dPHozEYhqHnn39ey5cv186dO1VTU6PGxka7ewEAgNByasOBk6eq0XCg9RF0WqBkX4kiTZFqMBoUaYpU6f7SoAg63377rSTp448/1scff+z0uiNHjkiSoqKi9M4772jmzJl67bXXlJ+fL0nq1q2b7r33XhUWFioyMvK0xxUfH9/sWFRU06PZ0NBgO1ZbWytJdtUcq6SkJI/fz9V9nN2rvLxc6enpqq2tVVZWlsaMGaP4+HhFRESotLRUGzZsaBZ8XPnNb36jZ555RikpKcrJyVFycrJiY2MlNTUw8OZeAAAg8G3M6K2sTRV2x05tG53JWpxWRdBpgay+WVq4eaEt7IxMHenvIXnEGijGjRunV1991aPXdOnSRU8//bSeeuop7dq1S++8846efvppFRUVKTo6WtOnT2/LIduxjv/gwYPNKidVVVUtuo8jju715JNP6rvvvtOf//xn/fKXv7Q7d9ddd2nDhg0ev//Bgwe1aNEiXXjhhSorK1P79u1t5yorKx1W2wAAQHBy1XCAKk7bYo1OC+Scm6PVN63Wb4b+JmimrUlNU9Hi4+P173//W8ePH/fqtSaTSf3799ekSZP0z3/+U5Ls2lFbKzsnV2Ba28CBAyVJ77/ffM7qxo3OWzKeKj4+Xn379lV5ebkqKyubnf/Xv/7V7Nhnn30mSbbOalaGYTgcj6vvx969e2UYhrKzs+1CjrP3BgAAwWljRm8NvWeuzqo+Ial5VzXaRrctgk4L5ZybowWjFwRNyJGapoPdfffd+vzzz/Xggw86DDs7d+60VTr2799v2/flZNaKR1xcnO1Y586dJUkHDhxog5E3uemmmxQREaEnnnhC1dXVtuNHjhzRnDnelXonTJig+vp6zZgxw+7422+/7XB9jrWCdGq768cee0w7d+5sdr2r74f1Xhs3brRbl/Pll1/6tEIGAADaBg0HAgNT18LMrFmztH37dj311FNas2aNLr/8cnXv3l0VFRX66KOP9OGHH6qsrEzdu3fXjh079Itf/ELp6em2hfvWvWMiIiI0efJk232tG4UWFBTo448/VkJCghITE21dxFrDueeeq2nTpmnu3LkaMGCAbrjhBkVFRWnlypUaMGCAdu7c6XGThKlTp2rlypVaunSpPv74Y11++eU6cOCA/va3v+naa6/VmjVr7K6/66679Pzzz2vcuHG64YYb1KVLF23atEnbt293eP15552nnj176pVXXlFsbKx69+4tk8mk++67z9ap7bXXXtOQIUN0xRVXqKqqSm+88YauuOIKW/UIAAAEHxoOBBBf9Lo+Xeyj0zKO9tExjKZ9av74xz8aw4YNM+Lj443Y2FjjzDPPNK6++mpjyZIlxg8//GAYhmEcOHDAmDZtmnHppZca3bt3N2JiYowzzzzT+MUvfmG3v43V8uXLjQEDBhixsbGGJKNPnz62c6720Tl1vxrDcL0PzeLFi43+/fsbMTExRu/evY0HH3zQOHDggCHJyM3N9fj788033xh33nmn0a1bNyMuLs4YPHiwsXLlSqfjKikpMYYNG2Z06tTJSExMNK655hpj27ZtTvcQ2rRpkzFixAijU6dOtr159u3bZxiGYRw+fNjIz883UlNTjdjYWCMtLc149NFHjfr6eq/2A3KFPxMAAPjOpsUFxkd92jfbD6fxv//c2y3K2LS4wN/DDAme7qNjMgzDcJiAAkhtba0SEhJUU1PjsEOXJB07dkz79u1T37597aZUITysW7dOV155paZOnar58+f7ezgBgT8TAAD4xqlVnFNZqzhoHZ5kA4k1Oggyhw4darbA//vvv7etbRk7dqwfRgUAAMKRdS2Oq5BDwwH/YY0Ogspf//pXFRcXa9SoUerZs6e+/vprrV27VgcPHtTEiROVkZHh7yECAIAwsDGjtzJP2RfnZDtTO+jI1PuVxd44fkPQQVDJzMzU4MGDtW7dOn377beKjIxU//799fDDD+uee+7x9/AAAECIc7UvjlXJ+Ew6qgUAgg6CSnp6ulavXu3vYQAAgDB0ahXn1JCzr1uUDs6aShUnQLBGBwAAAHDB3b4438c1VXH6HjyuoYScgEFFBwAAAHDC031xsnw+MrhDRQcAAAA4xeYlhdqZ2qFZRzVrFWdftyhtXlxAR7UARkUHAAAAOImrfXFOruL09e2w4CUqOgAAAMB/bczozb44IYKgAwAAgLDnrOGA1c7UDtq8uIC20UGEqWsAAAAIa64aDkjsixOsqOgAAAAgLHnacICQE5y8DjrvvvuuxowZo549e8pkMun11193+5rS0lJdfPHFio2N1dlnn63ly5e3YKgAAABA6yi5ZZiG3jNXF3x+tNk5a8MB9sUJbl4HnSNHjmjgwIFatGiRR9fv27dP1157rbKysrRjxw498MAD+vWvf6233nrL68EiOO3fv18mk0kTJ060Oz5y5EiZTKcWh1tPamqqUlNT2+z+AAAgONFwIDx4HXR+/vOfa/bs2bruuus8uv7ZZ59V37599cQTT6h///669957df311+vJJ5/0erBwzxoqTv6KiYlRSkqKxo8fr//85z/+HmKrmThxokwmk/bv3+/voQAAgCBAw4Hw0ubNCMrKypSdnW13bPTo0XrggQecvqaurk51dXW2X9fW1rbV8EJWv3799Mtf/lKS9MMPP2jTpk16+eWXtXLlSq1fv17Dhg3z8wilF198UUePNi8Xt5b169e32b0BAEBwoeFA+GnzoFNZWamkpCS7Y0lJSaqtrdWPP/6odu3aNXvNvHnzNGvWrLYeWkg7++yzNXPmTLtjDz30kObMmaPCwkKVlpb6ZVwnO/PMM9v0/v369WvT+wMAgMC3eUmhOsxfqKxT1uIYago7+7pF6eCsqcpiLU7ICciua9OnT1dNTY3t68CBA/4eUki47777JElbt26VJJlMJo0cOVIVFRW69dZb1aNHD0VERNiFIGvzia5duyo2NlZpaWl66KGHHFZiGhoaNH/+fJ199tmKi4vT2WefrXnz5qmxsdHheFyt0Vm9erWuuuoqdenSRXFxcUpNTdWECRO0c+dOSU3rb1544QVJUt++fW3T9EaOHGm7h7M1OkeOHFFRUZHOO+88xcXFqXPnzrr22mv1/vvN/y/OzJkzZTKZVFpaqpdeekmDBg1Su3btlJycrPvvv18//vhjs9e89tprGjFihLp37664uDj17NlT2dnZeu211xx+VgAA0DZoOBDe2ryi06NHD1VVVdkdq6qqUnx8vMNqjiTFxsYqNja2rYcWtk4OF998840yMjLUuXNn3XTTTTp27Jji4+MlSUuWLNGkSZOUmJioMWPGqHv37vr3v/+tOXPmqKSkRCUlJYqJibHd684779SyZcvUt29fTZo0SceOHdOCBQu0caPzxX6O5Ofna8GCBercubPGjh2r7t2768CBA1q3bp0GDx6sCy64QA888ICWL1+uDz/8UPfff78SExMlyW3zgWPHjmnUqFHasmWLLr74Yj3wwAOqqqrSihUr9NZbb+nll1+W2Wxu9rpnnnlGa9euVW5urkaNGqW1a9fqqaeeUnV1tf7617/arluyZInuueceJScn67rrrlOXLl1UWVmpLVu2aNWqVRo3bpxX3wsAANAyGzN6K2tThdPzTFULA8ZpkGSsWrXK5TVTp041LrjgArtjN998szF69GiP36empsaQZNTU1Di95scffzQ++eQT48cff/T4vqFo3759hiSH398ZM2YYkoysrCzDMJp+/yQZt912m3HixAm7az/++GMjKirKGDhwoFFdXW13bt68eYYko7i42HaspKTEkGQMHDjQ+OGHH2zHv/zyS6Nr166GJCMvL8/uPiNGjDBOfQT/8Y9/GJKMAQMGNHvf48ePG5WVlbZf5+XlGZKMffv2Ofxe9OnTx+jTp4/dsVmzZhmSjFtuucVobGy0Hd++fbsRExNjJCYmGrW1tbbjRUVFhiQjISHB2LVrl+340aNHjXPOOceIiIgwKioqbMcvvvhiIyYmxqiqqmo2nlM/T1vjzwQAIBxtWlxgfNY1yjAko1EyjFO+PkrtYGxaXODvYeI0eJINDMMwvJ669sMPP2jHjh3asWOHpKb20Tt27NAXX3whqWna2a233mq7/q677tLevXs1depU7dq1S4sXL9bf/vY3TZ48uYXRLEBYLNLkyU3/DEDl5eWaOXOmZs6cqd/+9re6/PLL9cgjjyguLk5z5vxUno2JidHjjz+uyMhIu9f/8Y9/1IkTJ/T000+rS5cuduemTp2qbt266eWXX7Yde/HFFyVJM2bMUIcOHWzHe/Xqpfvvv9/jcS9evFiS9Ic//KHZ+0ZFRTVb7+WtF154QdHR0XrsscfsKlsXXXSR8vLy9P333zvcG+r+++/Xueeea/t1u3btdPPNN6uxsVHbtm2zuzY6OlrR0dHN7nHq5wEAAK1rY0ZvDb1nrs6qPiHJccOBC/b9wFS1MOH11LV///vfysrKsv16ypQpkqS8vDwtX75cX3/9tS30SE3rJ9asWaPJkyfrD3/4g3r37q0//elPGj16dCsM308sFik3V4qMlBYulFavlnJy/D0qO5999pmtoUN0dLSSkpI0fvx4TZs2TQMGDLBd17dvX3Xt2rXZ6zdt2iRJeuuttxx2L4uOjtauXbtsv/7www8lScOHD292raNjzmzZskWxsbEaMWKEx6/xVG1trfbu3av+/furd+/ezc5nZWVp6dKl2rFjhyZMmGB3bvDgwc2ut97j+++/tx276aabNHXqVF1wwQUaP368srKydNlll9mmAwIAgNa3eUmhus14XJnVJ2xNBiQaDoQ7r4POyJEjZRiOOo83Wb58ucPXfPDBB96+VeAqKWkKOQ0NTf8sLQ24oDN69GitXbvW7XXOKiTffvutJNlVf1ypqalRRESEw9DkTRWmpqZGvXr1UkRE6/fJsLYpdzae5ORku+tO5iioREU1/fFpaGiwHXvwwQfVpUsXLVmyRE888YSKi4sVFRWla6+9Vk8++aT69u172p8DAAD8xFXbaGvDgcyyL8V/gcNPQHZdC3hZWT+FnIYG6aROX8HGWdcz6w/2tbW1MgzD6ZdVQkKCGhsbVV1d3exepzajcCUxMVGVlZVOO7WdDutncjaeyspKu+tawmQy6Ve/+pW2bt2qQ4cOadWqVfrFL36h1atX63/+53/sQhEAADg9GzN624UcSbaNQPd1i9LmxQXKLPvS9wNDQCDotEROTtN0td/8JiCnrbWGoUOHSvppCps7AwcOlCT961//anbO0TFn0tPTVVdXpw0bNri91rquyNPwEB8fr7POOkvl5eWqqGjehcXaVnvQoEEej9eVLl26aOzYsVqxYoVGjRqlTz75ROXl5a1ybwAAwtnmJYXa2y1amZsqdOo8I9pGw4qg01I5OdKCBSEZciTpnnvuUVRUlO677z67NVdW33//vd10ROualkceeURHjhyxHa+oqNAf/vAHj9930qRJkpoW/1unz1mdOHHCrhrTuXNnSfJqn6W8vDwdP35c06dPt6tI/ec//9Hy5cuVkJCgsWPHeny/U5WWljab2nn8+HHbZ4mLi2vxvQEAgGcNB6jiQPLBPjoIThdccIEWL16su+++W+eee66uueYa9evXT4cPH9bevXu1YcMGTZw4Uc8++6ykpoX8t912m55//nkNGDBA1113nerq6rRixQpdeumleuONNzx632uuuUYPPvigiouLlZaWpuuuu07du3dXRUWF1q9frwcffFAPPPCAJGnUqFEqLi7WnXfeqXHjxqlDhw7q06dPs0YCJ5s6darWrFmjP//5z/r00091xRVX6ODBg1qxYoVOnDihpUuXqlOnTi3+vo0dO1bx8fG69NJL1adPHx0/flz//Oc/9cknn+j6669Xnz59WnxvAADCWcktw3TRaxuVWScaDsAjBB04dccdd2jQoEFasGCB3n33Xf3jH/9QQkKCzjzzTE2ePFl5eXl21y9dulTnnHOOli5dqmeeeUa9e/fWlClTdMMNN3gcdCTp97//vTIyMvTMM8/o1Vdf1bFjx5ScnKxRo0bpyiuvtF3385//XI8//riWLl2qJ554QsePH9eIESNcBp24uDi98847mj9/vlasWKEnn3xS7du314gRI1RQUKDLLrvM+2/USebNm6e1a9dqy5Yt+sc//qEOHTqoX79+WrJkiW6//fbTujcAAOHI2lEt678VHImGA/CMyXDVQi1A1NbWKiEhQTU1NU4Xih87dkz79u1T3759mR4EiD8TAIDgtzGjtzI3NV9Xe2oVh7U44cWTbCBR0QEAAECAcbYvjhVVHHiCoAMAAICA4WpfHOmnKk4mVRy4QdABAABAQNiY0VtZp0xVczRNjSoOPEHQAQAAgF+5mqrGNDW0FEEHAAAAfnNqwwFH++Jk/fV93w4KIYENQwEAAOBzm5cUam+3aGVuqtDJLYCt/76vW5Q2Ly4g5KDFQq6iEwTdsgGf4M8CACBQuWo4wFQ1tJaQqehERkZKko4fP+7nkQCB4cSJpo3VoqJC7v9nAACC1OYlhdqZ2sEu5EjNqziZZV/6fnAIOSHzE1B0dLRiY2NVU1OjTp06yWQ6dYYnEF5qa2sVGRlp+58AAAD406lVnJNRxUFbCJmgI0ldu3ZVRUWFvvzySyUkJCg6OprAg7BjGIaOHDmi2tpaJScn82cAAOB3jtpGn4yGA2gLIRV04uPjJUnV1dWqqHD+hwkIdSaTSYmJiUpISPD3UAAAYcxV22hJ2pnaQUem3q8sNv9EGwipoCM1hZ34+HgdP35cDQ0N/h4O4BfR0dFMWQMA+JWrhgMSVRy0vZALOlbR0dGKjo729zAAAADCyuYlheowf6GyPj9qd9xa0dnXLUoHZ02lioM2F7JBBwAAAL5FwwEEkpBpLw0AAAD/2ZjR22nIkZqmqtE2Gr5E0AEAAECLbV5SqL3dopW5qUKOtqremdpBmxcXsB4HPsfUNQAAALTIxozeyjypbTQNBxBIqOgAAADAK86qONZ/39ctiioO/I6KDgAAADzmqm00DQcQSKjoAAAAwCOOGg6cWsWh4QACBRUdAAAAuLR5SaG6zXhcmdUnbPvhWFHFQaAi6AAAAMApGg4gWDF1DQAAAM3svtusmvYRNBxA0KKiAwAAAJvNSwrVo6hY5x6qtx1jqhqCEUEHAAAAkppPU7OyrsvZ1y1KB2dNVebdc3w+NsBbBB0AAIAw56rZgEQVB8GJoAMAABDGXO2LI0m7O0tbJl+vCQ/93bcDA04TQQcAACBMbczoraxTpqpZKzqfdY3UK3mDNeCOQk04N8cv4wNOB0EHAAAgzHi6L06hn8YHtAbaSwMAAISR+TcUaug9c3VW9QlJjvfFySz70vcDA1oZQQcAACAMWCxSWpoUs+X/U+NJx9kXB6GKoAMAABDiCgul3FypvFwqOfFzux8ArVPV+h48rqG0jUYIYY0OAABACDObpVdf/enX/6iYo5xe0q/0nDp2kDo9cDv74iAkEXQAAABCkMUi5ec3VXFO9Y+KOYq9fo7+TsdohDCmrgEAAIQYs/mnqWqOFBSIkIOQR9ABAAAIEdaGAydPVTtZWpq0erU0h5lqCANMXQMAAAgBhYXS3LnOz19/PVUchBcqOgAAAEHMYpHS052HHGsVh5CDcENFBwAAIEhRxQGco6IDAAAQhMxm1yGHhgMIdwQdAACAIOKu4UB6Og0HAImpawAAAEHD3VS1ggICDmBF0AEAAAhwFos0e7a0davj82lpUnGxlJPj23EBgYygAwAAEMBoOAC0DGt0AAAAAhQNB4CWI+gAAAAEGBoOAKePqWsAAAABxGx2HnAkGg4AnqKiAwAAEADcVXHS0qjiAN6gogMAAOBnNBwAWh8VHQAAAD+xVnGchRxrFYeQA3iPig4AAICPudsXR6KKA5wuKjoAAAA+VFgo5ea6Djm0jQZOHxUdAAAAH3HXUS0tTSoulnJyfDcmIFRR0QEAAGhj7jqqJSU1VXH27CHkAK2Fig4AAEAbYl8cwD+o6AAAALQB9sUB/IuKDgAAQCtjXxzA/wg6AAAArcjVVDWaDQC+Q9ABAABoBRaLlJ8vlZc7Pk8VB/At1ugAAACcJrO5aW8cZyGHfXEA3yPoAAAAtBANB4DAxdQ1AACAFqDhABDYqOgAAAB4wWKR0tOdhxxrFYeQA/gXFR0AAAAPUcUBggcVHQAAAA+Yza5DDg0HgMBC0AEAAHDBXcOB9HQaDgCBiKlrAAAATribqlZQQMABAhVBBwAA4BQWizR7trR1q+PzaWlScbGUk+PbcQHwHEEHAADgJDQcAEIDa3QAAAD+i4YDQOgg6AAAgLBHwwEg9DB1DQAAhDWz2XnAkWg4AAQrKjoAACAsuavipKVRxQGCGRUdAAAQdmg4AIQ+KjoAACBsWKs4zkKOtYpDyAGCHxUdAAAQ8tztiyNRxQFCDRUdAAAQ0goLpdxc1yGHttFA6KGiAwAAQpa7jmppaVJxsZST47sxAfANKjoAACDkuOuolpTUVMXZs4eQA4QqKjoAACCkuOuoxr44QHgg6AAAgJDgruEA09SA8ELQAQAAQY99cQCcijU6AAAgqJnN7qeqEXKA8EPQAQAAQcldw4H09KbNP1mPA4Qnpq4BAICg465tNA0HAFDRAQAAQcNdFSctjSoOgCZUdAAAQFCg4QAAbxB0AABAwHM1VY220QAcIegAAICAZbFI+flSebnj81RxADjDGh0AABCQzGYpN9d5yKFtNABXCDoAACCg0HAAQGtg6hoAAAgYNBwA0FpaVNFZtGiRUlNTFRcXp6FDh2rLli0ur1+4cKHOPfdctWvXTikpKZo8ebKOHTvWogEDAIDQY7E0bfDpLORYqziEHACe8rqis2LFCk2ZMkXPPvushg4dqoULF2r06NHavXu3unfv3uz6l156SdOmTdOyZcuUmZmpPXv2aOLEiTKZTFqwYEGrfAgAABC8qOIAaAteV3QWLFigO+64Q7fddpvOP/98Pfvss2rfvr2WLVvm8PqNGzdq2LBhGj9+vFJTU3XVVVfp5ptvdlsFAgAAoc9sdh1yaDgAoKW8Cjr19fXatm2bsrOzf7pBRISys7NVVlbm8DWZmZnatm2bLdjs3btXb775pq655hqn71NXV6fa2lq7LwAAEDrcNRxIT6fhAIDT49XUterqajU0NCgpKcnueFJSknbt2uXwNePHj1d1dbUuu+wyGYahEydO6K677lJBQYHT95k3b55mzZrlzdAAAECQcDdVraCAgAPg9LV5e+nS0lLNnTtXixcv1vbt27Vy5UqtWbNGjz76qNPXTJ8+XTU1NbavAwcOtPUwAQBAG/O04QAhB0Br8Kqi07VrV0VGRqqqqsrueFVVlXr06OHwNQ8//LAmTJigX//615KkAQMG6MiRI7rzzjtVWFioiIjmWSs2NlaxsbHeDA0AAAQwGg4A8DWvKjoxMTEaPHiw1q9fbzvW2Nio9evXKyMjw+Frjh492izMREZGSpIMw/B2vAAAIMjQcACAP3jdXnrKlCnKy8vTkCFDlJ6eroULF+rIkSO67bbbJEm33nqrevXqpXnz5kmSxowZowULFuiiiy7S0KFDVV5erocfflhjxoyxBR4AABBaLBbpT3+S/v1v6euvHV+Tnt5U6cnJ8e3YAIQHr4POjTfeqEOHDmnGjBmqrKzUoEGDtHbtWluDgi+++MKugvPQQw/JZDLpoYceUkVFhbp166YxY8ZoDhNwAQAISe6mqUk0HADQ9kxGEMwfq62tVUJCgmpqahQfH+/v4QAAACfMZucto6WmhgPFxVRxALScp9nA64oOAADAqSwWKT9fKi93fg0NBwD4Upu3lwYAAKGtsFDKzXUecqybfxJyAPgSFR0AANAiFos0e7a0davj80xTA+BPBB0AAOA19sUBEOiYugYAALzCvjgAggFBBwAAeMRiaZqO5qyrmnUtDm2jAQQCpq4BAAC33LWNZl8cAIGGig4AAHDKXRUnLY0qDoDAREUHAAA4RMMBAMGMoAMAAJpxNVWNttEAggFBBwAA2FgsUn6+880/qeIACBas0QEAAJKaqji5uc5DDm2jAQQTgg4AAGGOhgMAQhFT1wAACGM0HAAQqqjoAAAQhiyWpg0+nYUcaxWHkAMgWFHRAQAgzFDFARAOqOgAABBGzGbXIYeGAwBCBUEHAIAw4K7hQHo6DQcAhBamrgEAEOLcTVUrKCDgAAg9BB0AAEKUxSLNni1t3er4fFqaVFws5eT4dlwA4AsEHQAAQhANBwCEO9boAAAQYmg4AAAEHQAAQgYNBwDgJ0xdAwAgyLlbiyPRcABA+CHoAAAQxMxm5xUciYYDAMIXQQcAgCBksUj5+VJ5ufNraDgAIJyxRgcAgCBTWCjl5roOOTQcABDuqOgAABAk3K3FSU6WLrlEuv12pqoBAEEHAIAgwL44AOAdpq4BABDg2BcHALxH0AEAIECxLw4AtBxT1wAACEDupqqxLw4AuEbQAQAggLhrOMC+OADgGYIOAAABgoYDANB6WKMDAEAAoOEAALQugg4AAH5EwwEAaBtMXQMAwE/MZucBR6LhAACcDio6AAD4mLsqTloaVRwAOF1UdAAA8CEaDgCAbxB0AADwEVdT1WgbDQCti6ADAEAbs1ik/HypvNzxeao4AND6WKMDAEAbMpul3FznIYe20QDQNgg6AAC0ARoOAIB/MXUNAIBWRsMBAPA/KjoAALQSi6Vpg09nIcdaxSHkAEDbo6IDAEAroIoDAIGFig4AAKfJbHYdcmg4AAC+R9ABAKCF3DUcSE+n4QAA+AtT1wAAaAF3U9UKCgg4AOBPBB0AALxQWCg995xUVeX4fFqaVFws5eT4dlwAAHsEHQAAPGCxSPn5zjf+lGg4AACBhDU6AAC4UVgo5ea6Djk0HACAwELQAQDACXf74kg0HACAQMXUNQAAHHDXbIC1OAAQ2Ag6AACcwmx23jJaoqMaAAQDpq4BAPBf7IsDAKGDig4AAGJfHAAINQQdAEBYs1ik2bOlrVsdn2ctDgAEJ4IOACBsuavisC8OAAQv1ugAAMKS2ex+qhohBwCCF0EHABBWaDgAAOGBqWsAgLBBwwEACB8EHQBAyKPhAACEH4IOACCk0XAAAMITa3QAACGLhgMAEL4IOgCAkEPDAQAAU9cAACHFbHYecCQaDgBAuKCiAwAICe6qOGlpVHEAIJxQ0QEABD0aDgAATkXQAQAENVdT1WgbDQDhi6ADAAhKFouUny+Vlzs+TxUHAMIba3QAAEHHbJZyc52HHNpGAwAIOgCAoEHDAQCAp5i6BgAICjQcAAB4g4oOACCgFRZKPXo4DznWKg4hBwBwMio6AICA5K7ZgEQVBwDgHBUdAEDAKSx03WxAouEAAMA1KjoAgIDial8cSUpPbwpC7I0DAHCFig4AICB42lFt82ZCDgDAPSo6AAC/c1fFKSigZTQAwDtUdAAAfsO+OACAtkJFBwDgF+yLAwBoS1R0AAA+ZbE0NRRgXxwAQFuiogMA8BmqOAAAX6GiAwDwCbPZdchhXxwAQGsi6AAA2pS7hgPp6TQcAAC0PqauAQDajLuparSNBgC0FYIOAKDVWSzS7NnS1q2Oz6elScXFbPwJAGg7BB0AQKui4QAAIBCwRgcA0GpoOAAACBQEHQDAaaPhAAAg0DB1DQBwWsxm5wFHouEAAMA/qOgAAFrEXRUnLY0qDgDAf6joAAC8RsMBAECgI+gAALziaqoabaMBAIGCoAMA8IjFIuXnS+Xljs9TxQEABBLW6AAA3DKbpdxc5yGHttEAgEBD0AEAOEXDAQBAsGpR0Fm0aJFSU1MVFxenoUOHasuWLS6v//777zVp0iQlJycrNjZW55xzjt58880WDRgA4BuFha6rONdfL+3Zw3ocAEBg8jrorFixQlOmTFFRUZG2b9+ugQMHavTo0Tp48KDD6+vr63XllVdq//79evXVV7V7924tXbpUvXr1Ou3BAwBan8XStMGns65q1ioOU9UAAIHMZBiG4c0Lhg4dqksuuUTPPPOMJKmxsVEpKSm67777NG3atGbXP/vss/r973+vXbt2KTo6ukWDrK2tVUJCgmpqahQfH9+iewAAXLNYpNmzpa1bnV9DwwEAgL95mg28qujU19dr27Ztys7O/ukGERHKzs5WWVmZw9dYLBZlZGRo0qRJSkpK0gUXXKC5c+eqoaHB6fvU1dWptrbW7gsA0Has09RchRwaDgAAgolXQae6uloNDQ1KSkqyO56UlKTKykqHr9m7d69effVVNTQ06M0339TDDz+sJ554QrNnz3b6PvPmzVNCQoLtKyUlxZthAgC8YDa73vwzPZ2GAwCA4NPmXdcaGxvVvXt3/e///q8GDx6sG2+8UYWFhXr22Wedvmb69OmqqamxfR04cKCthwkAYcddRzWpqYqzeTMNBwAAwcerDUO7du2qyMhIVVVV2R2vqqpSjx49HL4mOTlZ0dHRioyMtB3r37+/KisrVV9fr5iYmGaviY2NVWxsrDdDAwB4wWx2HXDS05umsxFwAADByquKTkxMjAYPHqz169fbjjU2Nmr9+vXKyMhw+Jphw4apvLxcjY2NtmN79uxRcnKyw5ADAGg7nu6LQxUHABDsvJ66NmXKFC1dulQvvPCCPv30U9199906cuSIbrvtNknSrbfequnTp9uuv/vuu/Xtt9/q/vvv1549e7RmzRrNnTtXkyZNar1PAQBwi31xAADhxKupa5J044036tChQ5oxY4YqKys1aNAgrV271tag4IsvvlBExE/5KSUlRW+99ZYmT56sCy+8UL169dL999+v3/3ud633KQAALrmaqpaWJhUXE3AAAKHF6310/IF9dACgZSwWKT/fdRWHltEAgGDSJvvoAACCh9nseqoa++IAAEIZQQcAQoynDQfYFwcAEMq8XqMDAAhchYWuN/9kqhoAIFxQ0QGAEGCxNO194yzkWKs4hBwAQLigogMAQY4qDgAAzVHRAYAgZja7Djk0HAAAhCuCDgAEIXcNB9LTaTgAAAhvTF0DgCDjbqpaQQEBBwAAgg4ABAmLRZo9W9q61fH5tDSpuFjKyfHtuAAACEQEHQAIAjQcAADAO6zRAYAAR8MBAAC8R9ABgABFwwEAAFqOqWsAEIDMZucBR6LhAAAA7lDRAYAA4q6Kk5ZGFQcAAE9Q0QGAAEHDAQAAWg9BBwACgKuparSNBgDAewQdAPAji0XKz5fKyx2fp4oDAEDLsEYHAPzAuhYnN9d5yKFtNAAALUdFBwB8zF1HNaaqAQBw+gg6AOAj7qapSUxVAwCgtRB0AMAHqOIAAOBbrNEBgDbkbl+cxMSmtTh79hByAABoTVR0AKCNsC8OAAD+Q9ABgDbAvjgAAPgXQQcAWhH74gAAEBhYowMArcRsZl8cAAACBUEHAE6Tu4YDaWnS6tXSnDm+HRcAAOGMqWsAcBpoOAAAQGCiogMALWCxSOnpzkOOtYpDyAEAwD+o6ACAl6jiAAAQ+KjoAIAXzGbXIYeGAwAABAaCDgB4wF3DgfR0Gg4AABBImLoGAG64m6pWUEDAAQAg0BB0AMAJi0WaPVvautXx+bQ0qbhYysnx7bgAAIB7BB0AcICGAwAABDfW6ADAKWg4AABA8CPoAMB/0XAAAIDQwdQ1AFBTFcdZwJFoOAAAQLChogMgrLmr4qSlUcUBACAYUdEBELZoOAAAQOgi6AAIS66mqtE2GgCA4EfQARBWLBYpP18qL3d8nioOAAChgTU6AMKG2Szl5joPObSNBgAgdBB0AIS8wkIpMZGGAwAAhBOmrgEIWe6mqUlMVQMAIFRR0QEQkgoLXU9Ts1ZxCDkAAIQmKjoAQorFIs2eLW3d6vwaqjgAAIQ+KjoAQoa1iuMs5FDFAQAgfFDRARASXO2LIzV1VKPZAAAA4YOKDoCgZrE0VWqchZz0dDqqAQAQjqjoAAhaVHEAAIAzVHQABB13VRz2xQEAAFR0AASVwkJp7lzn5+moBgAAJIIOgCDiaqpaWppUXCzl5Ph2TAAAIDARdAAEPItFys93vvknVRwAAHAq1ugACGhmc9PeOM5CTkEBIQcAADRH0AEQkGg4AAAATgdT1wAEHBoOAACA00VFB0DAsFiaNvh0FnKsVRxCDgAAcIeKDoCAQBUHAAC0Jio6APzObHYdcmg4AAAAvEXQAeA37hoOpKfTcAAAALQMU9cA+IW7qWoFBQQcAADQcgQdAD5lsUizZ0tbtzo+n5YmFRdLOTm+HRcAAAgtBB0APkPDAQAA4Cus0QHgEzQcAAAAvkTQAdCmaDgAAAD8galrANqM2ew84Eg0HAAAAG2Hig6AVldYKCUmOg85aWlUcQAAQNuiogOg1VgsUn6+VF7u/BoaDgAAAF+gogOgVRQWSrm5zkOOtYpDyAEAAL5ARQfAaXG3L45EFQcAAPgeFR0ALWat4rja/JMqDgAA8AcqOgBahI5qAAAgkFHRAeAV9sUBAADBgIoOAI9RxQEAAMGCig4At9xVcdgXBwAABBoqOgBcKiyU5s51fp6OagAAIBARdAA45WqqWlqaVFws5eT4dkwAAACeIOgAaMZikfLznW/+SRUHAAAEOtboALBjNjftjeMs5BQUEHIAAEDgI+gAkETDAQAAEFqYugaAhgMAACDkUNEBwpjF0rTBp7OQY63iEHIAAECwoaIDhCmqOAAAIJRR0QHCkNnsOuTQcAAAAAQ7gg4QRtw1HEhPp+EAAAAIDUxdA8KEu6lqBQUEHAAAEDoIOkCIs1ik2bOlrVsdn09Lk4qLpZwc344LAACgLRF0gBBGwwEAABCuWKMDhCgaDgAAgHBG0AFCDA0HAAAAmLoGhBQaDgAAADQh6AAhgIYDAAAA9gg6QBBzF3AkGg4AAIDwxBodIEgVFkq5ua5DDg0HAABAuCLoAEHGYmlqKOBqLQ4NBwAAQLhj6hoQRNw1G5BoOAAAACC1sKKzaNEipaamKi4uTkOHDtWWLVs8et0rr7wik8mksWPHtuRtgbDmbl8cqjgAAAA/8TrorFixQlOmTFFRUZG2b9+ugQMHavTo0Tp48KDL1+3fv18PPvighg8f3uLBAuHI031xNm+mqxoAAICV10FnwYIFuuOOO3Tbbbfp/PPP17PPPqv27dtr2bJlTl/T0NCgW265RbNmzdJZZ511WgMGwom14UB5uePzBQUEHAAAAEe8Cjr19fXatm2bsrOzf7pBRISys7NVVlbm9HWPPPKIunfvrttvv92j96mrq1Ntba3dFxBO3DUcSEtjmhoAAIArXjUjqK6uVkNDg5KSkuyOJyUladeuXQ5f89577+m5557Tjh07PH6fefPmadasWd4MDQgZ7hoOsC8OAACAe23aXvrw4cOaMGGCli5dqq5du3r8uunTp6umpsb2deDAgTYcJRA43DUcYF8cAAAAz3hV0enatasiIyNVVVVld7yqqko9evRodv1nn32m/fv3a8yYMbZjjY2NTW8cFaXdu3erX79+zV4XGxur2NhYb4YGBDWLRcrPd74WJz29qdLDWhwAAADPeFXRiYmJ0eDBg7V+/XrbscbGRq1fv14ZGRnNrj/vvPP00UcfaceOHbavnJwcZWVlaceOHUpJSTn9TwAEObOZhgMAAACtzesNQ6dMmaK8vDwNGTJE6enpWrhwoY4cOaLbbrtNknTrrbeqV69emjdvnuLi4nTBBRfYvT4xMVGSmh0Hwo27Kk5amlRcTMABAABoCa+Dzo033qhDhw5pxowZqqys1KBBg7R27Vpbg4IvvvhCERFtuvQHCHo0HAAAAGhbJsMwDH8Pwp3a2lolJCSopqZG8fHx/h4OcFrMZuebf1LFAQAAcM3TbOB1RQdAy7ibqkYVBwAAoPUwxwzwAU8aDhByAAAAWg9BB2hDFkvTdDRXU9VWr5bmzPHtuAAAAEIdU9eANkLDAQAAAP+hogO0MoulaYNPZyHHWsUh5AAAALQdKjpAK6KKAwAAEBio6ACtxGx2HXJoOAAAAOA7BB3gNLlrOJCeTsMBAAAAX2PqGnAa3E1VKygg4AAAAPgDQQdoAYtFmj1b2rrV8fm0NKm4WMrJ8e24AAAA0ISgA3iJhgMAAACBjzU6gIesa3FoOAAAABD4CDqAG9Z9cXJzpfJyx9fQcAAAACCwMHUNcMHdNDWJhgMAAACBiKADOGE2O28ZLdFwAAAAIJAxdQ04hbt9cZKSmqo4e/YQcgAAAAIVFR3gJOyLAwAAEBoIOoDYFwcAACDUEHQQ9tgXBwAAIPSwRgdhzWxmXxwAAIBQRNBBWHLXcIB9cQAAAIIbU9cQdty1jabhAAAAQPCjooOw4a6Kk5ZGFQcAACBUUNFBWKDhAAAAQHgh6CDkuZqqRttoAACA0ETQQciyWKT8fKm83PF5qjgAAAChizU6CElms5Sb6zzk0DYaAAAgtBF0EFJoOAAAAACJqWsIITQcAAAAgBVBByGBhgMAAAA4GUEHQY2GAwAAAHCENToIWjQcAAAAgDMEHQQdGg4AAADAHaauIajQcAAAAACeoKKDoGCxSOnpzkOOtYpDyAEAAIBERQdBgCoOAAAAvEVFBwHNbHYdcmg4AAAAAEcIOghIhYVSYqLzhgPp6TQcAAAAgHNMXUNAcbcvjtRUxSHgAAAAwBWCDgKG2ey8giM1NRwoLpZycnw3JgAAAAQngg78zpMqDg0HAAAA4A3W6MCvCgul3FznIYe20QAAAGgJgg78wtN9cfbsYaoaAAAAvMfUNfgc++IAAACgrVHRgU+xLw4AAAB8gaADn7BYmqajsS8OAAAAfIGpa2hz7qaqsS8OAAAAWhtBB23GYpFmz5a2bnV8nn1xAAAA0FYIOmgTNBwAAACAP7FGB62OhgMAAADwN4IOWg0NBwAAABAomLqGVmE2Ow84Eg0HAAAA4FtUdHBa3FVx0tKo4gAAAMD3qOigxWg4AAAAgEBF0EGLuJqqRttoAAAA+BtBB16xWKT8fKm83PF5qjgAAAAIBKzRgcfMZik313nIoW00AAAAAgVBB27RcAAAAADBhqlrcImGAwAAAAhGVHTgkMXStMGns5BjreIQcgAAABCIqOigGao4AAAACHZUdGDHbHYdcmg4AAAAgGBA0IEk9w0H0tNpOAAAAIDgwdS1MGexSLNnS1u3Or+moICAAwAAgOBC0AljZrPzCo7UVOEpLpZycnw3JgAAAKA1EHTCkMUi5ec73/hTouEAAAAAghtrdMJMYaGUm+s65NBwAAAAAMGOik6YcLcWJzlZuuQS6fbbmaoGAACA4EfQCQPsiwMAAIBww9S1EMe+OAAAAAhHBJ0Qxb44AAAACGdMXQtB7qaqsS8OAAAAQh1BJ4S4azjAvjgAAAAIFwSdEEHDAQAAAOAnrNEJATQcAAAAAOwRdIIYDQcAAAAAx5i6FqRoOAAAAAA4R9AJMjQcAAAAANwj6AQRGg4AAAAAnmGNTpCg4QAAAADgOYJOgKPhAAAAAOA9pq4FMLPZecCRaDgAAAAAOENFJwC5q+KkpVHFAQAAAFyhohNgaDgAAAAAnD6CTgBxNVWNttEAAACA5wg6AcBikfLzpfJyx+ep4gAAAADeYY2On5nNUm6u85BD22gAAADAewQdP6HhAAAAANB2mLrmYxaLNHu2tHWr82uYqgYAAACcHoKOD7nbF4eGAwAAAEDrIOj4gLtmAxJVHAAAAKA1sUanjRUWum42INFwAAAAAGhtVHTakKupasnJ0iWXSLffzlQ1AAAAoLURdNoA++IAAAAA/sXUtVbGvjgAAACA/xF0Wgn74gAAAACBg6lrraCwUJo71/l5pqoBAAAAvkVF5zRYLFJ6uvOQY63iEHIAAAAA32pR0Fm0aJFSU1MVFxenoUOHasuWLU6vXbp0qYYPH64zzjhDZ5xxhrKzs11eHyysbaO3bnV8/vrrpT176KgGAAAA+IPXQWfFihWaMmWKioqKtH37dg0cOFCjR4/WwYMHHV5fWlqqm2++WSUlJSorK1NKSoquuuoqVVRUnPbg/cVsdj1VjYYDAAAAgH+ZDMMwvHnB0KFDdckll+iZZ56RJDU2NiolJUX33Xefpk2b5vb1DQ0NOuOMM/TMM8/o1ltv9eg9a2trlZCQoJqaGsXHx3sz3Fbnam+c9PSmSg9VHAAAAKBteJoNvKro1NfXa9u2bcrOzv7pBhERys7OVllZmUf3OHr0qI4fP67OnTs7vaaurk61tbV2X4HAYnEecgoKpM2bCTkAAABAIPAq6FRXV6uhoUFJSUl2x5OSklRZWenRPX73u9+pZ8+edmHpVPPmzVNCQoLtKyUlxZthtpmSEslksj9G22gAAAAg8Pi069pjjz2mV155RatWrVJcXJzT66ZPn66amhrb14EDB3w4SueysiTD+Cns0HAAAAAACExe7aPTtWtXRUZGqqqqyu54VVWVevTo4fK1xcXFeuyxx7Ru3TpdeOGFLq+NjY1VbGysN0PziZycpupNaak0ciQBBwAAAAhUXlV0YmJiNHjwYK1fv952rLGxUevXr1dGRobT1z3++ON69NFHtXbtWg0ZMqTlow0AOTnSggWEHAAAACCQeVXRkaQpU6YoLy9PQ4YMUXp6uhYuXKgjR47otttukyTdeuut6tWrl+bNmydJmj9/vmbMmKGXXnpJqamptrU8HTt2VMeOHVvxowAAAABAE6+Dzo033qhDhw5pxowZqqys1KBBg7R27Vpbg4IvvvhCERE/FYqWLFmi+vp6XX/99Xb3KSoq0syZM09v9AAAAADggNf76PhDIO2jAwAAAMB/2mQfHQAAAAAIBgQdAAAAACGHoAMAAAAg5BB0AAAAAIQcgg4AAACAkEPQAQAAABByCDoAAAAAQg5BBwAAAEDIIegAAAAACDkEHQAAAAAhh6ADAAAAIOQQdAAAAACEHIIOAAAAgJBD0AEAAAAQcgg6AAAAAEIOQQcAAABAyIny9wA8YRiGJKm2ttbPIwEAAADgT9ZMYM0IzgRF0Dl8+LAkKSUlxc8jAQAAABAIDh8+rISEBKfnTYa7KBQAGhsb9dVXX6lTp04ymUx+HUttba1SUlJ04MABxcfH+3UsCA48M/AWzwy8xTMDb/HMwFuB9MwYhqHDhw+rZ8+eiohwvhInKCo6ERER6t27t7+HYSc+Pt7vv8kILjwz8BbPDLzFMwNv8czAW4HyzLiq5FjRjAAAAABAyCHoAAAAAAg5BB0vxcbGqqioSLGxsf4eCoIEzwy8xTMDb/HMwFs8M/BWMD4zQdGMAAAAAAC8QUUHAAAAQMgh6AAAAAAIOQQdAAAAACGHoAMAAAAg5BB0AAAAAIQcgo4DixYtUmpqquLi4jR06FBt2bLF5fV///vfdd555ykuLk4DBgzQm2++6aORIlB488wsXbpUw4cP1xlnnKEzzjhD2dnZbp8xhB5v/56xeuWVV2QymTR27Ni2HSACjrfPzPfff69JkyYpOTlZsbGxOuecc/jvU5jx9plZuHChzj33XLVr104pKSmaPHmyjh075qPRwt/effddjRkzRj179pTJZNLrr7/u9jWlpaW6+OKLFRsbq7PPPlvLly9v83F6g6BzihUrVmjKlCkqKirS9u3bNXDgQI0ePVoHDx50eP3GjRt188036/bbb9cHH3ygsWPHauzYsdq5c6ePRw5/8faZKS0t1c0336ySkhKVlZUpJSVFV111lSoqKnw8cviLt8+M1f79+/Xggw9q+PDhPhopAoW3z0x9fb2uvPJK7d+/X6+++qp2796tpUuXqlevXj4eOfzF22fmpZde0rRp01RUVKRPP/1Uzz33nFasWKGCggIfjxz+cuTIEQ0cOFCLFi3y6Pp9+/bp2muvVVZWlnbs2KEHHnhAv/71r/XWW2+18Ui9YMBOenq6MWnSJNuvGxoajJ49exrz5s1zeP0NN9xgXHvttXbHhg4davy///f/2nScCBzePjOnOnHihNGpUyfjhRdeaKshIsC05Jk5ceKEkZmZafzpT38y8vLyjNzcXB+MFIHC22dmyZIlxllnnWXU19f7aogIMN4+M5MmTTJGjRpld2zKlCnGsGHD2nScCEySjFWrVrm8ZurUqcbPfvYzu2M33nijMXr06DYcmXeo6Jykvr5e27ZtU3Z2tu1YRESEsrOzVVZW5vA1ZWVldtdL0ujRo51ej9DSkmfmVEePHtXx48fVuXPnthomAkhLn5lHHnlE3bt31+233+6LYSKAtOSZsVgsysjI0KRJk5SUlKQLLrhAc+fOVUNDg6+GDT9qyTOTmZmpbdu22aa37d27V2+++aauueYan4wZwScYfgaO8vcAAkl1dbUaGhqUlJRkdzwpKUm7du1y+JrKykqH11dWVrbZOBE4WvLMnOp3v/udevbs2ewvC4Smljwz7733np577jnt2LHDByNEoGnJM7N371698847uuWWW/Tmm2+qvLxc99xzj44fP66ioiJfDBt+1JJnZvz48aqurtZll10mwzB04sQJ3XXXXUxdg1POfgaura3Vjz/+qHbt2vlpZD+hogP40WOPPaZXXnlFq1atUlxcnL+HgwB0+PBhTZgwQUuXLlXXrl39PRwEicbGRnXv3l3/+7//q8GDB+vGG29UYWGhnn32WX8PDQGqtLRUc+fO1eLFi7V9+3atXLlSa9as0aOPPurvoQEtRkXnJF27dlVkZKSqqqrsjldVValHjx4OX9OjRw+vrkdoackzY1VcXKzHHntM69at04UXXtiWw0QA8faZ+eyzz7R//36NGTPGdqyxsVGSFBUVpd27d6tfv35tO2j4VUv+nklOTlZ0dLQiIyNtx/r376/KykrV19crJiamTccM/2rJM/Pwww9rwoQJ+vWvfy1JGjBggI4cOaI777xThYWFiojg/43DnrOfgePj4wOimiNR0bETExOjwYMHa/369bZjjY2NWr9+vTIyMhy+JiMjw+56SfrnP//p9HqElpY8M5L0+OOP69FHH9XatWs1ZMgQXwwVAcLbZ+a8887TRx99pB07dti+cnJybF1uUlJSfDl8+EFL/p4ZNmyYysvLbaFYkvbs2aPk5GRCThhoyTNz9OjRZmHGGpQNw2i7wSJoBcXPwP7uhhBoXnnlFSM2NtZYvny58cknnxh33nmnkZiYaFRWVhqGYRgTJkwwpk2bZrv+/fffN6Kioozi4mLj008/NYqKiozo6Gjjo48+8tdHgI95+8w89thjRkxMjPHqq68aX3/9te3r8OHD/voI8DFvn5lT0XUt/Hj7zHzxxRdGp06djHvvvdfYvXu38cYbbxjdu3c3Zs+e7a+PAB/z9pkpKioyOnXqZLz88svG3r17jbffftvo16+fccMNN/jrI8DHDh8+bHzwwQfGBx98YEgyFixYYHzwwQfG559/bhiGYUybNs2YMGGC7fq9e/ca7du3N377298an376qbFo0SIjMjLSWLt2rb8+QjMEHQeefvpp48wzzzRiYmKM9PR0Y9OmTbZzI0aMMPLy8uyu/9vf/macc845RkxMjPGzn/3MWLNmjY9HDH/z5pnp06ePIanZV1FRke8HDr/x9u+ZkxF0wpO3z8zGjRuNoUOHGrGxscZZZ51lzJkzxzhx4oSPRw1/8uaZOX78uDFz5kyjX79+RlxcnJGSkmLcc889xnfffef7gcMvSkpKHP58Yn1O8vLyjBEjRjR7zaBBg4yYmBjjrLPOMp5//nmfj9sVk2FQjwQAAAAQWlijAwAAACDkEHQAAAAAhByCDgAAAICQQ9ABAAAAEHIIOgAAAABCDkEHAAAAQMgh6AAAAAAIOQQdAAAAACGHoAMAAAAg5BB0AAAAAIQcgg4AAACAkPP/Aw5+0zUpSIo9AAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 1000x700 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "with torch.inference_mode():\n",
    "    y_preds = model(X_test)\n",
    "\n",
    "    plot_predictions(predictions=y_preds.cpu())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save and Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yolov7",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
